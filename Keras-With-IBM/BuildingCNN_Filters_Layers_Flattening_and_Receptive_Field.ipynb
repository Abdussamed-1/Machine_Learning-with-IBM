{"cells":[{"cell_type":"markdown","id":"86eaf8df-e94c-4887-8190-23ffcb7e8a84","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n","</center>\n"]},{"cell_type":"markdown","id":"3085713d-c861-4881-851d-492d98b2fe62","metadata":{},"outputs":[],"source":["# **Building a CNN: Filters, Layers, Flattening, and Receptive Field**\n"]},{"cell_type":"markdown","id":"7ca09cc4-bc0f-40d1-a719-1541820f981b","metadata":{},"outputs":[],"source":["Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","id":"ea60fa97-9072-4371-b0c9-dda17466e377","metadata":{},"outputs":[],"source":["Now that you have an idea of how filters can be applied to a channel in different ways via padding, stride, and activation functions, let's proceed to the next level. What do we do when there are multiple filters for the channel or multiple layers of filters ? Like a regular neural network with multiple neurons, in a CNN you can have  multiple convolution layers. In addition, you can have multiple filters or kernels per layer. In this lab, we will see how a CNN processes multiple channels, filters, and layers, and flatten the output so it can be used as input to a regular neural network.\n"]},{"cell_type":"markdown","id":"e5a13b32-8279-4063-b312-7b467360c674","metadata":{},"outputs":[],"source":["## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Deeper-Networks-and-the-Receptive-Field:-Relating-Input-to-Output\">Deeper Networks and the Receptive Field: Relating Input to Output</a>\n","        <ol>\n","            <li><a href=\"#What-is-it-for?\">What is it for?</a></li>\n","            <li><a href=\"#What-affects-receptive-field?\">What affects receptive field?</a></li>\n","            <li><a href=\"#Model-1:-No-Stride\">Model 1: No Stride</a></li>\n","            <li><a href=\"#Model-2:-Increased-Stride\">Model 2: Increased Stride</a></li>\n","            <li><a href=\"#Model-3:-Increased-Layers\">Model 3: Increased Layers</a></li>\n","            <li><a href=\"#Visualizing-the-Outputs\">Visualizing the Outputs</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"#Varied-Input-Output-Relationships\">Varied Input-Output Relationships</a></li>\n","        <ol>\n","            <li><a href=\"#Import-the-Data-and-Define-Filters\">Import the Data and Define Filters</a></li>\n","            <li><a href=\"#Single-Input-Multiple-Outputs\">Single Input, Multiple Outputs</a></li>\n","            <li><a href=\"#Multiple-Inputs-Single-Output\">Multiple Inputs, Single Output</a></li>\n","            <li><a href=\"#Multiple-Inputs-Multiple-Outputs\">Multiple Inputs, Multiple Outputs</a></li>\n","        </ol>\n","    <li><a href=\"#Deeper-CNNs\">Deeper CNNs</a></li>\n","        <ol>\n","            <li><a href=\"#Why-do-we-want-multiple-layers?\">Why do we want multiple layers?</a></li>\n","            <li><a href=\"#Example:-Preparing-a-CNN-for-Flower-Classification\">Example: Preparing a CNN for Flower Classification</a></li>\n","            <li><a href=\"#Flattening\">Flattening</a></li>\n","            <li><a href=\"#Dense-Layers\">Dense Layers</a></li>\n","        </ol>\n","</ol>\n"]},{"cell_type":"markdown","id":"e4a5e4fd-70ac-4e47-bdc3-14ead2ac1738","metadata":{},"outputs":[],"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n"," - Understand the relationship between features and effective receptive field size\n"," - Apply filters on inputs of varying depth and calculate output size\n"," - Build CNNs with greater depth to learn more features and prepare for image classification\n"]},{"cell_type":"markdown","id":"b7f4b3c2-8b82-47c6-b413-5beeb1a7353a","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"524e7e5f-b62d-410e-aaad-634fdc26ad34","metadata":{},"outputs":[],"source":["## Setup\n"]},{"cell_type":"markdown","id":"2a74022a-3369-4aa4-9a83-5d3f5fdbf5a6","metadata":{},"outputs":[],"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools\n"]},{"cell_type":"markdown","id":"2eccf11d-c691-4a14-a202-b15ee392b54b","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the folowing code cell.\n"]},{"cell_type":"code","id":"be4edda5-af52-435a-8ced-4b2519b343af","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"markdown","id":"e5f74ba9-d9c7-45c5-8dbe-18b15beb31c2","metadata":{},"outputs":[],"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"markdown","id":"15a4af20-74fb-4f78-89a8-38105891a6e3","metadata":{},"outputs":[],"source":["### Importing Required Libraries\n","\n","_We recommend you import all required libraries in one place (here):_\n"]},{"cell_type":"code","id":"f0e2e9af-7d7f-4dde-b6b1-78926445708d","metadata":{},"outputs":[],"source":["# You can also use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\n\n\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # tensorflow INFO and WARNING messages are not printed "]},{"cell_type":"code","id":"2e774c86-aab1-4023-aa21-855c85ea8a43","metadata":{},"outputs":[],"source":["import pathlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nimport PIL\nfrom PIL import Image, ImageOps\nimport tensorflow as tf\n\nimport glob\nfrom tensorflow import keras\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense"]},{"cell_type":"markdown","id":"beac2623-e2cd-452b-afdd-168432e8db09","metadata":{},"outputs":[],"source":["### Defining Helper Functions\n"]},{"cell_type":"code","id":"9dc91f54-438e-46da-8041-69cc9fbbc5d2","metadata":{},"outputs":[],"source":["def calc_rf(model):\n    # Initialize an array storing all the layers' receptive field sizes and set layer 0 (input) RF to 1\n    num_layers = len(model.layers)\n    rf_arr = np.empty(num_layers+1, dtype=int)\n    rf_arr[0] = 1\n    # Initialize an array storing all the layers' jump sizes and set layer 0 (input) jump to 1\n    j_arr = np.empty(num_layers+1, dtype=int)\n    j_arr[0] = 1\n    \n    for i in range(num_layers):\n        layer = model.layers[i]\n        k = layer.kernel_size[0]\n        s = layer.strides[0]\n        j_in = j_arr[i]\n        j_out = j_in * s\n        r_out = rf_arr[i] + (k - 1) * j_in\n        j_arr[i+1] = j_out\n        rf_arr[i+1] = r_out        \n        print(\"Layer {}: {} \\n Jump Size: {}\\n Effective Receptive Field Size: {}\".format(i+1, layer.name, j_arr[i+1], rf_arr[i+1]))\n        print(\"------\")"]},{"cell_type":"markdown","id":"421f11ed-9518-4ada-b013-f1f005fd8969","metadata":{},"outputs":[],"source":["## Deeper Networks and the Receptive Field: Relating Input to Output\n"]},{"cell_type":"markdown","id":"d0792d78-4dec-46ab-b459-8eee4c7e29ba","metadata":{},"outputs":[],"source":["Why should we add more layers to a CNN? It turns out that CNNs behave like the human eye. When we look at something, each of the million neurons in our eye is responsible for observing a specific section of the view. Similarly in a CNN, each output feature is responsible for gathering information in a region of the input. This region is equal to the size of the kernel that it is convolved with to get a single value in the output. We call this region the feature's **receptive field**. \n","\n","As more layers are added to the network, we also use the term **“effective receptive field”** (ERF) to refer to the pixels in the first input, the original image, that was indirectly convolved to attain a feature in a subsequent layer. Hence, it makes sense that ERF size increases with layers as each feature is generated from a convolution of its input, each pixel of which is a convolution of the previous layer, and so on. \n"]},{"cell_type":"markdown","id":"c7156814-713e-4b79-9af3-e4493c9e2255","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/images/rf_fixed.png\" width=\"600\" alt=\"visual showing receptive field with fixed map size\">\n","<center>\n","    \n","Image credits to [Dang Ha The Hien](https://blog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)\n"]},{"cell_type":"markdown","id":"358a1647-305d-41e8-84fa-dc3f8b0382e0","metadata":{},"outputs":[],"source":["In the visual above, the shaded region that surrounds the center feature in the top two layers is their ERF. You can see that a feature in the green layer has an ERF size of 3x3, meaning that it observes those 9 pixels in the input image (blue). In the yellow layer, this has increased - one feature is responsible for viewing a 7x7 region in the input. Notably, the center feature's ERF covers almost all of the input pixels!\n"]},{"cell_type":"markdown","id":"766f0e2d-cf4a-4e80-9148-f5c07d5baa01","metadata":{},"outputs":[],"source":["We can also use a formula to calculate what the effective receptive field size is:\n","\n","$$r_{out} = r_{in} + (k - 1)*j_{in}$$\n","\n","- $r_{out}$: ERF width/height of an output feature\n","- $r_{in}$: ERF width/height of an input feature\n","- $k$: convolution kernel width/height\n","- $j_{in}$: distance between two consecutive features in the input\n","- Note: $j_{out} = j_{in} * s$ for $s$ = stride size\n","\n","As an example, let's apply convolution to the input image (blue) above with k=3 and calculate the ERF. In the input, each feature's ERF size is 1 because it only observes itself. The distance between two features is 1. Hence, in our first layer output (green), the ERF size is 1 + (3-1)\\*1 = 3.\n"]},{"cell_type":"markdown","id":"676d43a1-62bc-469b-9530-a0b90cda0141","metadata":{},"outputs":[],"source":["### What is it for?\n"]},{"cell_type":"markdown","id":"7ed65255-d657-4766-8c8a-810ceac902f0","metadata":{},"outputs":[],"source":["Receptive field describes how much information is learned by each layer of the network. The bigger the receptive field size, the bigger “view” each feature has. Let’s think back to the eye analogy. If a neuron is only able to observe a small region, then 1) one neuron might not have enough information to detect a bigger object, and 2) we need more neurons to attain a complete view. In CNN tasks such as object detection and image segmentation, this is detrimental; features won't be able to learn to recognize a characteristic spanning a bigger input region.\n"]},{"cell_type":"markdown","id":"9ca3039e-e899-4bf5-8bef-57baf0c3880a","metadata":{},"outputs":[],"source":["### What affects receptive field?\n"]},{"cell_type":"markdown","id":"7d322521-5254-4a5a-a511-4dd5b03e0323","metadata":{},"outputs":[],"source":["Ideally, we want each output feature to have a big receptive field so that crucial information is captured and that relevant features can be completely recognized. Think of it like a camera - if you zoom in too much, the lens can only see parts of an object, which could prevent you from detecting what it is. Things that impact receptive field size include:\n","\n","- Number of convolution layers: making a deeper network increases field size linearly by kernel size.\n","- Stride: as a method of sub-sampling, the higher the stride value, the fewer input pixels shared between neighboring features, the bigger the receptive field as we add more layers.\n","- Pooling: this is also a way of sub-sampling that increases the size multiplicatively. It causes resolution loss as the relationship between the original input features isn't fully retained.\n","- Dilation rate: introduces spaces in between kernel values such that they aren't applied to adjacent samples. Increases size exponentially without loss of resolution like sub-sampling techniques (not covered).\n"]},{"cell_type":"markdown","id":"09c06b3f-12ce-4cbb-8135-0de680671978","metadata":{},"outputs":[],"source":["Let's see an example of how stride and adding layers affect the receptive field! First let's download the image:\n"]},{"cell_type":"code","id":"879d965a-ba94-46b0-b385-f08f2ee7ef22","metadata":{},"outputs":[],"source":["!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/images/RF_dot.png\""]},{"cell_type":"markdown","id":"3be1d29a-f577-44f1-b9b4-38a3853541e0","metadata":{},"outputs":[],"source":["We load the image:\n"]},{"cell_type":"code","id":"417f4272-8dc5-42d8-9085-88c294281147","metadata":{},"outputs":[],"source":["img_width = 300\nimg_height = 300\n\ndot = PIL.Image.open(\"RF_dot.png\")\ndot = ImageOps.grayscale(dot)\n# dot = PIL.ImageOps.invert(dot)\ndot = dot.resize((img_width, img_height))\nplt.imshow(dot, cmap=\"gray\")\nplt.show()"]},{"cell_type":"markdown","id":"e632892f-1e34-4c0a-b785-40adae744706","metadata":{},"outputs":[],"source":["Below, we define the kernel and bias weights. For this section, we chose to use the average kernel, which takes the mean of the pixels. Also, we will use the helper function ```calc_rf``` defined previously to determine the ERF size of a feature in each layer.\n"]},{"cell_type":"code","id":"9e8c9596-8e7f-4a09-8fd5-5cb6e2efd769","metadata":{},"outputs":[],"source":["kernel = (1/9)*np.ones((3,3,1,1))\nb = np.array([0.0])"]},{"cell_type":"markdown","id":"7bdaa079-8a1a-492b-a70e-5c971ebb8523","metadata":{},"outputs":[],"source":["### Model 1: No Stride\n","To begin, we'll check what the output looks like when we have a model with no stride. With the parameters specified, our ERF should be 3 for the output.\n"]},{"cell_type":"code","id":"2fd2e732-363c-4eca-bf98-62b3b221f744","metadata":{},"outputs":[],"source":["# Construct the model with no stride\nmodel1 = Sequential()\n\nmodel1.add(Conv2D(input_shape = (img_width, img_height, 1),\n                 filters=1, \n                 kernel_size=(3,3)\n                 ))\n\nmodel1.layers[0].set_weights([kernel,b])\n\ncalc_rf(model1)"]},{"cell_type":"markdown","id":"25ce4dde-69c3-40a8-8154-639281dc40e0","metadata":{},"outputs":[],"source":["### Model 2: Increased Stride\n"]},{"cell_type":"markdown","id":"c84841c7-9dca-452f-b73c-d5ec839f5f01","metadata":{},"outputs":[],"source":["Here we set the ```strides``` to 2 for the 2nd model.\n"]},{"cell_type":"code","id":"85fddc2b-09cb-4380-9bb7-42adf9166f04","metadata":{},"outputs":[],"source":["# Construct the model with stride = 2\nmodel2 = Sequential()\n\nmodel2.add(Conv2D(input_shape = (img_width, img_height, 1),\n                 filters=1, \n                 kernel_size=(3,3), \n                 padding='same',\n                 strides=2\n                 ))\n\nmodel2.layers[0].set_weights([kernel,b])\n\ncalc_rf(model2)"]},{"cell_type":"markdown","id":"f239de0b-ea69-41e5-9a6e-348617c41b6c","metadata":{},"outputs":[],"source":["Even with increased stride size, the ERF size is still 3. This is expected because our output's ERF size depends on the kernel and the input's RF and jump, all of which haven't changed between model 1 and 2. \n"]},{"cell_type":"markdown","id":"d81db4b8-1155-4f4d-8aa0-3607f62acb40","metadata":{},"outputs":[],"source":["### Model 3: Increased Layers\n"]},{"cell_type":"markdown","id":"4b874e33-cf15-419e-ad11-efcc1ed281e2","metadata":{},"outputs":[],"source":["Lets see what happens with we increased the number of layers:\n"]},{"cell_type":"code","id":"c6aeb662-d68c-42bb-a28d-41ef863364a2","metadata":{},"outputs":[],"source":["# Construct the model with multiple layers\nmodel3 = Sequential()\n\nmodel3.add(Conv2D(input_shape = (img_width, img_height, 1),\n                 filters=1, \n                 kernel_size=(3,3)\n                 ))\nmodel3.add(Conv2D(filters=1, \n                 kernel_size=(3,3), \n                 strides=4\n                 ))\nmodel3.add(Conv2D(filters=1, \n                 kernel_size=(3,3), \n                 strides=5\n                 ))\n\nmodel3.layers[0].set_weights([kernel,b])\nmodel3.layers[1].set_weights([kernel,b])\nmodel3.layers[2].set_weights([kernel,b])\n\ncalc_rf(model3)"]},{"cell_type":"markdown","id":"db9cc3dc-8385-4565-82c2-39a1b15c1409","metadata":{},"outputs":[],"source":["After adding more layers to the model, we clearly see the ERF size for each feature increased in subsequent layers. In particular, the ERF of layer 3 is significantly bigger than it would have been if there's no stride.\n"]},{"cell_type":"markdown","id":"333b862b-2710-4c38-a3e7-7201eff1e61a","metadata":{},"outputs":[],"source":["### Visualizing the Outputs\n"]},{"cell_type":"markdown","id":"53a035ad-8fa5-4f0f-a425-838724bfdc04","metadata":{},"outputs":[],"source":["Let's compare the results to of the models:\n"]},{"cell_type":"code","id":"b0384dc3-1f48-4be1-ad01-a0b0cd990804","metadata":{},"outputs":[],"source":["models = [model1, model2, model3]\ndot_tensor = np.array(dot).reshape((1,img_width,img_height,1))\nfig, ax = plt.subplots(1,len(models))\n\nfor i in range(len(models)):\n    plt.subplot(1,len(models),i+1)\n    output = models[i].predict(dot_tensor)\n    output = output.reshape(output.shape[1],output.shape[2])\n    plt.title(\"Model {}\".format(i+1))\n    plt.imshow(output, cmap='gray')"]},{"cell_type":"markdown","id":"51691928-9498-428d-8504-0c920df9f99e","metadata":{},"outputs":[],"source":["As we increase the receptive field the subsequent images get smaller and smaller, but we can still make out the shape. This also decreased the number of parameters needed in the model.\n"]},{"cell_type":"markdown","id":"44ccfc10-aa02-412d-8225-d97b2e1810cc","metadata":{},"outputs":[],"source":["## Varied Input-Output Relationships\n"]},{"cell_type":"markdown","id":"0d4d4d78-0125-4534-8eef-eca3d029b28d","metadata":{},"outputs":[],"source":["We previously only dealt with applying one kernel on one channel at a time, but that is quite inefficient to do since we often have access to multichannel (colored) image inputs, whose information can be learned jointly. Moreover, the convolutional layers are analogous to the hidden neurons in a neural network, and like a regular neural network, adding more neurons will improve results. Next, we will take a look at how increasing the number of channels can increase the information extracted.\n"]},{"cell_type":"markdown","id":"9e8deee7-faa2-4075-8d8e-0a0d03452137","metadata":{},"outputs":[],"source":["Consider the following image:\n"]},{"cell_type":"code","id":"01417ac3-276a-4981-b3e6-dd49ae938fed","metadata":{},"outputs":[],"source":["!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/images/channel_image.jpg\""]},{"cell_type":"code","id":"a3553691-ca0e-4f4e-8229-1b6203319383","metadata":{},"outputs":[],"source":["img_width = 300\nimg_height = 300\n\nimage = PIL.Image.open(\"channel_image.jpg\").resize((img_width, img_height))\nimage"]},{"cell_type":"markdown","id":"1e778161-33f0-457b-be8c-1970af64dcf7","metadata":{},"outputs":[],"source":["First, we define two filters we will use throughout the examples. The first filter is an edge kernel, which you have seen previously in the lab on padding, strides, and activation. This kernel helps us detect edges in the image. The second filter is a kernel that sharpens the image through creating greater contrast. Note that usually, in CNN, we would let the network learn the best filters automatically instead of defining them.\n"]},{"cell_type":"code","id":"128d17b6-80dc-45a7-982a-d19f277348ef","metadata":{},"outputs":[],"source":["def edge_grad(shape, dtype=None):\n    grad = np.array([\n        [-1, -1, -1],\n        [-1, 8, -1],\n        [-1, -1, -1]\n        ]).reshape(shape)\n    \n    return grad\n\ndef sharpen_grad(shape, dtype=None):\n    grad = np.array([\n        [0, -1, 0],\n        [-1, 5, -1],\n        [0, -1, 0]\n    ]).reshape(shape)\n    \n    return grad"]},{"cell_type":"markdown","id":"8d74c027-0b87-436d-9959-62ea62e68999","metadata":{},"outputs":[],"source":["### Single Input, Multiple Outputs\n"]},{"cell_type":"markdown","id":"2add9c29-9536-4f5a-a479-8469c20a6dbf","metadata":{},"outputs":[],"source":["Based on what you learned from past labs, you know how to apply a filter on a black and white image, so let's start with that. However, this time we will apply multiple filters on the grayscale image. \n"]},{"cell_type":"markdown","id":"a6ed6483-19c2-4bd2-80a3-24c8d3841105","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/images/SingleInMultiOut.png\" width=\"600\" alt=\"single input multiple output calculation\">\n","<center>\n"]},{"cell_type":"markdown","id":"c388af06-5e41-4273-85a3-d992a0609fb5","metadata":{},"outputs":[],"source":["The visual above, shows how outputs are calculated when there is a single channel but multiple filters. We can see that the depth of the output (number of channels it has) corresponds to the number of filters applied on the input. \n"]},{"cell_type":"markdown","id":"4ac96a4e-6635-431d-a2ad-fe6037362636","metadata":{},"outputs":[],"source":["Here's a formula for calculating the width/height of one output channel:\n","$$n_{out} = \\frac{n_{in} + 2p - k}{s} + 1$$\n","\n","- $n_{out}$: width/height of output map\n","- $n_{in}$: width/height of input map\n","- $p$: convolution padding size\n","- $k$: convolution kernel width/height\n","- $s$: convolution stride size\n"]},{"cell_type":"markdown","id":"d623990c-15ca-434c-8eb9-bd82e3515cd9","metadata":{},"outputs":[],"source":["As a first step, we convert the image to grayscale such that we work with a single channel as our input for now. Here, when the image is converted to grayscale, the stars are not visible anymore due to their RGB values being converted to a singular value representing brightness, which raises the concern of whether the filters will be able to detect any features.\n"]},{"cell_type":"code","id":"d86d3171-e9e6-498f-8f92-09212675eff8","metadata":{},"outputs":[],"source":["gray = ImageOps.grayscale(image)\nimg_tensor = np.array(gray).reshape((1, img_width, img_height, 1)) # Convert to a tensor for model prediction\ngray"]},{"cell_type":"markdown","id":"559fb237-0a8a-4a63-9ed0-23025956a7fd","metadata":{},"outputs":[],"source":["Next, we define the desired kernels. Note here that the kernel takes on the size 3x3x1x2. This means that there are 2 filters, each with the dimension 3x3x1, where 1 represents the number of channels and 3x3 is the 2D kernel size. This shape is required because we are performing both convolutions in a single layer. The bias, set to 0 for both convolutions on the input, is represented by the variable `b`.\n"]},{"cell_type":"code","id":"0db3a27b-eede-4e73-80e6-b325b733a7e0","metadata":{},"outputs":[],"source":["kernels = np.array([sharpen_grad((3,3,1,1)), edge_grad((3,3,1,1))])\nkernels = kernels.reshape((3,3,1,2))\nb = np.array([np.array([0.0]),np.array([0.0])]).reshape(2,)"]},{"cell_type":"markdown","id":"9e8a313a-cbca-4233-859c-2665441802a7","metadata":{},"outputs":[],"source":["Here, we define the Sequential model, adding one convolutional layer with 2 filters of size 3x3. After this, we specify the kernels and biases by the function `set_weights`. To see more information about the model built, use `summary()`.\n"]},{"cell_type":"code","id":"cd47fdb9-1795-4de8-ae31-ad0d2411cf7b","metadata":{},"outputs":[],"source":["model = Sequential()\n\nmodel.add(Conv2D(filters = 2, \n                 kernel_size = (3, 3), \n                 padding='same', \n                 input_shape = (img_width, img_height, 1)))\n\nmodel.set_weights([kernels,b])\n\nmodel.summary()"]},{"cell_type":"markdown","id":"84447335-3bf5-48b1-b854-e1fc40884c71","metadata":{},"outputs":[],"source":["In order to see how each filter is applied on the input, we use an additional model that specifies the desired output when given input from our previous model.\n"]},{"cell_type":"code","id":"d5504c02-3fd1-42e7-a810-391ffa956bc3","metadata":{},"outputs":[],"source":["layer_outputs = [layer.output for layer in model.layers] # Extracts the outputs of the layer\nactivation_model = Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n\nactivation_model.summary()"]},{"cell_type":"markdown","id":"b9c0fae4-60d2-4ae9-b760-ff4f3ed0c184","metadata":{},"outputs":[],"source":["Now that we have set up the model structure, we feed the image tensor as input to the activation model to separate the outputs from individual kernels.\n"]},{"cell_type":"code","id":"02a2777b-f97a-463a-ae1d-446eb680850d","metadata":{},"outputs":[],"source":["activations = activation_model.predict(img_tensor) # Obtain the outputs (activations) from our model"]},{"cell_type":"code","id":"ace2144f-a7bb-49d8-89d0-68b0916e2b62","metadata":{},"outputs":[],"source":["filter_outputs = activations[0]  # Extract the outputs from the first (in this case, the only) layer of the model\nprint(filter_outputs.shape)"]},{"cell_type":"markdown","id":"d417ff15-5af5-43f0-96af-2d1dcaf13d27","metadata":{},"outputs":[],"source":["The output shape confirms that the model correctly applied the two filters on the input as the output depth is 2 - one dimension for each filter. Using the formula above, we also know that the width and height are (1)*300+2(1)-3 + 1 = 300. Since each filter output is a tensor, we can also display their weights by directly indexing the object:\n"]},{"cell_type":"code","id":"80e9d37c-9319-4aba-aa3a-d49acdc46006","metadata":{},"outputs":[],"source":["# Display weights of the first filter of this layer\nfilter_outputs[:,:,0]"]},{"cell_type":"markdown","id":"1851dd8e-a345-40bb-827b-b1a7d14e45fe","metadata":{},"outputs":[],"source":["We can also visualize the outputs from each convolution:\n"]},{"cell_type":"code","id":"e62bd190-cf6a-4377-a394-b562c05a6ada","metadata":{},"outputs":[],"source":["# Display the filter outputs\nnames = [\"Sharpening Filter\", \"Edge Filter\"]\nfor i in range(2):\n    plt.subplot(1,2,i+1)\n    plt.imshow(filter_outputs[:,:,i], cmap='gray')\n    plt.axis(\"off\")\n    plt.title(f\"{names[i]}\", fontsize=13)"]},{"cell_type":"markdown","id":"0eb57f21-9c5b-4140-a0c6-92a1a2f285ad","metadata":{},"outputs":[],"source":["We see that given a grayscale image in which the shapes blended in completely with the background, the sharpening and edge filters can't detect any features that were apparent in the colored image.\n"]},{"cell_type":"markdown","id":"b4a36aed-f512-4727-aad4-a7ae7fba9c33","metadata":{},"outputs":[],"source":["### Multiple Inputs, Single Output\n"]},{"cell_type":"markdown","id":"e1935f2c-8705-4e33-971a-e00f925643f6","metadata":{},"outputs":[],"source":["So, what if we used a colored image instead? Will the network be better at sharpening the image? Before moving onto utilizing multiple filters, let's see how the output is calculated when we have one filter applied on multiple channels, such as RGB.\n"]},{"cell_type":"markdown","id":"62292024-61a0-4d0a-84e0-c09200234c2d","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/images/MultiInSingleOut.png\" width=\"600\" alt=\"multiple input single output calculation\">\n","<center>\n"]},{"cell_type":"markdown","id":"f7cce5b6-1aea-41e8-bedb-3e60bd17ad85","metadata":{},"outputs":[],"source":["How a filter is applied on multi-channel input is quite similar to the single channel case. What's different is that now, the depth of the filter will correspond to the number of input channels, so one 2D kernel is designated for one channel. The convolved result for each channel is then added together to form a single output channel.\n"]},{"cell_type":"markdown","id":"ccb09a6e-538b-4e06-89f8-cc969e0c6e76","metadata":{},"outputs":[],"source":["#### Exercise: Reshape the Image\n"]},{"cell_type":"code","id":"aeb6770c-3ee6-448f-abf2-270c2cc6dd77","metadata":{},"outputs":[],"source":["# Using the RGB image from before, convert it to a numpy array\n# and reshape it to be 300x300x3, where 3 is for the three channels.\n# WRITE YOUR CODE HERE\nimg_tensor = np.array(image).reshape((1, img_width, img_height, 3))"]},{"cell_type":"markdown","id":"f14ddb51-f806-4b52-af61-119de492a5ef","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","    <code>img_tensor = np.array(image).reshape((1, img_width, img_height, 3))</code>\n","</details>\n"]},{"cell_type":"markdown","id":"8d2c3d4e-702a-447c-ab7c-d2f5241123a0","metadata":{},"outputs":[],"source":["Again, we define the filter we hope to use. In this case, the kernel is of shape 3x3x3x1 because we want one 3x3 filter for each of the 3 input channels. Note that these three filters take on the same value since we want to apply the same convolution on all the channels. With one filter involved for convolution, we specify one bias value.\n"]},{"cell_type":"code","id":"af1ad918-8fe4-41c7-a794-db8f0019b239","metadata":{},"outputs":[],"source":["kernel = np.array([sharpen_grad((3,3,1,1)), sharpen_grad((3,3,1,1)), sharpen_grad((3,3,1,1))])\nkernel = kernel.reshape((3,3,3,1))\nb = np.array([0.0])"]},{"cell_type":"code","id":"7ee45fc3-e073-4d00-9482-5bd1636ef3bb","metadata":{},"outputs":[],"source":["model = Sequential()\n\nmodel.add(Conv2D(filters = 1, \n                 kernel_size = (3, 3), \n                 padding='same', \n                 input_shape = (img_width, img_height, 3)))\n\nmodel.set_weights([kernel,b])\n\nmodel.summary()"]},{"cell_type":"markdown","id":"f3ca6d4c-c30d-491c-a46f-809a5ff582d0","metadata":{},"outputs":[],"source":["The output shape for the layer can also be seen in the model summary above. It matches our expectations based on the padding, stride, kernel size, and number of kernels. Again, we can visualize the output result. Since we only have a single filter, we don't need an additional model for separating out the filter outputs. However, we do need to reshape the predicted output into 2D to be able to plot it.\n"]},{"cell_type":"code","id":"89486a83-6e39-460f-bbc8-dc26ce9a067e","metadata":{},"outputs":[],"source":["output = model.predict(img_tensor)\noutput = output.reshape((img_width, img_height))"]},{"cell_type":"markdown","id":"641eff5a-84f1-4a99-a535-42d4db5fd20d","metadata":{},"outputs":[],"source":["In this case, `output` directly shows us the result of applying a filter on the multichannel input.\n"]},{"cell_type":"code","id":"17a888f8-2719-4590-801a-5b5f9c66f963","metadata":{},"outputs":[],"source":["# View values of the output tensor\noutput"]},{"cell_type":"code","id":"08133410-9778-4311-91cc-229fec1fd7c7","metadata":{},"outputs":[],"source":["plt.imshow(output, cmap='gray')"]},{"cell_type":"markdown","id":"57202c08-db65-454d-be3c-2d67d33f762a","metadata":{},"outputs":[],"source":["It is evident here that applying a single filter on the colored image already performs better than applying two filters on the grayscale. You can see the stars now!\n"]},{"cell_type":"markdown","id":"0b8ffc15-914c-4614-9f94-ca7be38ca73b","metadata":{},"outputs":[],"source":["### Multiple Inputs, Multiple Outputs\n"]},{"cell_type":"markdown","id":"efe64a7f-a84a-468b-ad23-ec2c7a797f41","metadata":{},"outputs":[],"source":["Now for the most complex relationship -  applying multiple filters on multichannel inputs all at once. How is convolution performed, and what would the output look like? Combining knowledge from the previous two subsections, you can probably guess how we calculate the output based on multichannel inputs with multiple filters. Each output channel is calculated the same way as the \"Multiple Inputs, Single Output\" case. The number of output channels corresponds to the number of filters applied.\n"]},{"cell_type":"markdown","id":"5cd0c18e-baf6-4455-abc1-795d9ec56e9c","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/images/MultiInMultiOut.png\" width=\"600\" alt=\"multiple input multiple output calculation\">\n","<center>\n"]},{"cell_type":"markdown","id":"55d31654-d12c-434a-8466-20204cdbdaf9","metadata":{},"outputs":[],"source":["It is evident in the visual above that the number of output channels correspond to the number of filters we have, and each channel is the summation of convolution on the input channels for a single kernel. Let's apply the method to a color image.\n"]},{"cell_type":"code","id":"768d1137-23c8-4a6b-a2cc-3365917509e3","metadata":{},"outputs":[],"source":["img_tensor = np.array(image).reshape((1, img_width, img_height, 3))"]},{"cell_type":"markdown","id":"9f9a371d-111e-4f38-98df-745bb59fa057","metadata":{},"outputs":[],"source":["We will take our color image and apply the filters to get two outputs. Here, we have 3 input channels x 2 outputs, meaning that we need 6 filter channels total, 3 for each filter. We define the filter here:\n"]},{"cell_type":"code","id":"45e6e249-a406-48b0-9a67-6c6282c0e06a","metadata":{},"outputs":[],"source":["kernel = np.zeros((3,3,3,2))\n\nkernel[0] = np.dstack([sharpen_grad((3,3,1)),edge_grad((3,3,1))])\nkernel[1] = np.dstack([sharpen_grad((3,3,1)),edge_grad((3,3,1))])\nkernel[2] = np.dstack([sharpen_grad((3,3,1)),edge_grad((3,3,1))])\n\nkernel = kernel.reshape((3,3,3,2))"]},{"cell_type":"markdown","id":"b61c744a-d99c-42db-9c03-3ddc6ff167ac","metadata":{},"outputs":[],"source":["#### Exercise: Define biases \n","If we have one bias per operation on the input, what does our bias object look like?  Set the bias to 0.\n"]},{"cell_type":"code","id":"42b00c47-14da-4d48-a8d1-05d68757f09c","metadata":{},"outputs":[],"source":["# WRITE YOUR CODE HERE\nb = np.array([np.array([0.0]),np.array([0.0])]).reshape(2,)"]},{"cell_type":"markdown","id":"583d4d60-c3b1-4af8-ba7a-8221659ef0ea","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","    <code>b = np.array([np.array([0.0]),np.array([0.0])]).reshape(2,)</code>\n","</details>\n"]},{"cell_type":"markdown","id":"fc2b7873-5e12-4aec-baaa-5570b12735a1","metadata":{},"outputs":[],"source":["We proceed to define the model:\n"]},{"cell_type":"code","id":"0e487dc2-0fec-497f-807c-e0e5c401a99f","metadata":{},"outputs":[],"source":["model = Sequential()\n\nmodel.add(Conv2D(filters = 2, \n                 kernel_size = (3, 3), \n                 padding='same', \n                 input_shape = (img_width, img_height, 3)))\n\nmodel.set_weights([kernel,b])\n\nmodel.summary()"]},{"cell_type":"markdown","id":"1cd8a49c-7241-4c57-a919-c7c6c7afab4d","metadata":{},"outputs":[],"source":["We assign the the pre-made filters:\n"]},{"cell_type":"code","id":"0a015ec5-cb3d-4822-9dd8-2fe67ad9e03c","metadata":{},"outputs":[],"source":["layer_outputs = [layer.output for layer in model.layers] \nactivation_model = Model(inputs=model.input, outputs=layer_outputs) \nactivations = activation_model.predict(img_tensor)\nfilter_outputs = activations[0]"]},{"cell_type":"markdown","id":"6c4c17ca-57fa-4f61-8e71-5450925e77b0","metadata":{},"outputs":[],"source":["Here, you can see that no matter how many channels we have as input, the output dimension doesn't depend on it.\n"]},{"cell_type":"code","id":"6b54a4b0-8bfd-4336-8ff3-39aaf33ef69a","metadata":{},"outputs":[],"source":["plt.imshow(filter_outputs[:,:,0],cmap='gray') # Print out the convolved result of the first filter"]},{"cell_type":"code","id":"b5ba0a69-4d08-4971-a988-c941dd5f464d","metadata":{},"outputs":[],"source":["# Display the filter outputs\nnames = [\"Sharpening Filter\", \"Edge Filter\"]\nfor i in range(2):\n    plt.subplot(1,2,i+1)\n    plt.imshow(filter_outputs[:,:,i], cmap='gray')\n    plt.axis(\"off\")\n    plt.title(f\"{names[i]}\", fontsize=13)"]},{"cell_type":"markdown","id":"dc67007a-2433-4ca8-b0e3-584b2cf35bb1","metadata":{},"outputs":[],"source":["From the two outputs, we notice that they produce very similar results, which is reasonable considering how simple the shape edges are.\n"]},{"cell_type":"markdown","id":"b462d1a7-82a1-4aee-b335-c8a2b66b1977","metadata":{},"outputs":[],"source":["## Deeper CNNs\n"]},{"cell_type":"markdown","id":"95b99172-0bdf-411c-911a-37dc2e5ba3eb","metadata":{},"outputs":[],"source":["In the last lab, you had a preview of multi-layered CNNs. Now with a better understanding of the relationship between input and output size, we can decipher where the output shapes of each layer come from when training the network.\n"]},{"cell_type":"markdown","id":"8b5eb37c-0881-4269-b4ca-698bca20fdae","metadata":{},"outputs":[],"source":["### Why do we want multiple layers?\n"]},{"cell_type":"markdown","id":"11626b80-d197-4d81-8849-3bd3027e6985","metadata":{},"outputs":[],"source":["The benefit of multiple layers is that each layer can be responsible for detecting a certain set of features, which will help train the model better than if we only had a single layer. Moreover, we can specify different activation functions for each convolution layer. Let's see that in action with an example.\n"]},{"cell_type":"markdown","id":"ba730020-8ab5-43be-aff7-e38ae16552dd","metadata":{},"outputs":[],"source":["### Example: Preparing a CNN for Flower Classification\n"]},{"cell_type":"markdown","id":"c1a02cc3-f844-46e6-b20b-3b90a16b9dfe","metadata":{},"outputs":[],"source":["We will use our flower images from the image convolution lab. However, this time, we want to use them to train a CNN model for classification. Hence, we will utilize a training and test set.\n","https://www.tensorflow.org/tutorials/load_data/images\n"]},{"cell_type":"code","id":"6f4a5552-c7a2-4387-8d07-155ce64809c7","metadata":{},"outputs":[],"source":["dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/flower_photos.tgz\"\ndata_dir = keras.utils.get_file(origin=dataset_url,\n                                fname='flower_photos',\n                                untar=True)\n\ndata_dir = pathlib.Path(data_dir)\n\nfor folder in data_dir.glob('[!LICENSE]*'):\n    print('The', folder.name, 'folder has',\n          len(list(folder.glob('*.jpg'))), 'pictures')\nimage_count = len(list(data_dir.glob('*/*.jpg')))\nprint(image_count, 'total images')"]},{"cell_type":"code","id":"3b437a58-5db8-4a1f-b4bb-94799ec50d1b","metadata":{},"outputs":[],"source":["pics = list()\npics_arr = list()\np_class = list()\n\nimg_width = 300\nimg_height = 300\n\nplt.figure(figsize=(20,5))\nfor idx, folder in enumerate(data_dir.glob('[!LICENSE]*')):\n    cat = list(data_dir.glob(folder.name + '/*'))\n    pic = PIL.Image.open(str(cat[0])).resize((img_width, img_height))\n    pic_arr = np.array(pic)\n    clss = folder.name\n    \n    plt.subplot(1,5,idx+1)\n    plt.imshow(pic)\n    plt.title(clss)\n    plt.axis('off')\n    \n    pics.append(pic)\n    pics_arr.append(pic_arr)\n    p_class.append(clss)\n    "]},{"cell_type":"markdown","id":"5455c760-7940-40f7-b15d-682091503cba","metadata":{},"outputs":[],"source":["#### Exercise: Define the model and add the first layer\n","\n","Define the Sequential model and add the following layer to our classifier:\n","1. Convolutional layer with 3 input channels, three 3x3 filters, even padding, and relu activation function.\n"]},{"cell_type":"code","id":"fd4dcce0-e85a-41a0-9c8b-bbb14545f14f","metadata":{},"outputs":[],"source":["# WRITE YOUR CODE HERE\nclassifier = Sequential()\nclassifier.add(Conv2D(3, (3, 3), padding='same', input_shape = (img_width, img_height, 3), activation = 'relu'))\n"]},{"cell_type":"markdown","id":"ccb3a693-0a1f-4c3c-a7db-91902bfaed0f","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","    <code>classifier.add(Conv2D(3, (3, 3), padding='same', input_shape = (img_width, img_height, 3), activation = 'relu'))</code></br>\n","</details>\n"]},{"cell_type":"markdown","id":"ee33e004-11fd-4252-9c06-641aca086356","metadata":{},"outputs":[],"source":["#### Exercise: Add additional layers\n","In Keras, you can simply call `add()` to add multiple layers to the Sequential model you've defined. \n","1. Convolutional layer with two 3x3 filters and sigmoid activation function.\n","2. Convolutional layer with six 5x5 filters, stride = 4, even padding, and relu activation function.\n","3. Max pooling layer with size 2x2.\n"]},{"cell_type":"code","id":"b40fdc37-4fc0-4776-bb52-ab1910974f9d","metadata":{},"outputs":[],"source":["# WRITE YOUR CODE HERE\nclassifier.add(Conv2D(2, (3, 3), activation='sigmoid'))\nclassifier.add(Conv2D(6, (5, 5), strides = 4, padding='same', activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size=(2, 2))) \n"]},{"cell_type":"markdown","id":"b65be5bc-9e63-4117-9a48-bd5434806496","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","    <code>classifier.add(Conv2D(2, (3, 3), activation='sigmoid'))</code></br>\n","    <code>classifier.add(Conv2D(6, (5, 5), strides = 4, padding='same', activation = 'relu'))</code></br>\n","    <code>classifier.add(MaxPooling2D(pool_size=(2, 2))) </code>\n","</details>\n"]},{"cell_type":"markdown","id":"e6e3a1cc-3b91-4e49-ab03-d00130034a10","metadata":{},"outputs":[],"source":["### Flattening\n"]},{"cell_type":"markdown","id":"3df3f060-601d-4cb2-9dbe-c474c74716c2","metadata":{},"outputs":[],"source":["When we get here, we've almost finished constructing the CNN model! Flattening is the process of converting input into a 1D array for the next layer. The reason behind this is that our final task - classification - isn't compatible with higher dimensional input, considering we desire a class label as output. \n","\n","The way that tensors are flattened is quite simple. We desire an output of the shape Nx1, where N is the number of features in the input. For 2-dimensional input, this is straightforward - a tensor of the shape 28x28 will flatten into a single vector of 784x1. \n","\n","What happens when the input has multiple channels? The shape is calculated in a similar way; that is, a 28x28x3 3D tensor would flatten to be 2352x1. Each channel is flattened first and then lined up along a single axis. \n"]},{"cell_type":"code","id":"9bf1214f-e303-4f51-939c-0541bd1c04ec","metadata":{},"outputs":[],"source":["# Add a flattening layer\nclassifier.add(Flatten()) "]},{"cell_type":"code","id":"2739ce50-a506-4c68-904e-56c547961f91","metadata":{},"outputs":[],"source":["classifier.summary()"]},{"cell_type":"markdown","id":"384f0b71-96f0-4ad2-9fbc-6c0e2bdea440","metadata":{},"outputs":[],"source":["From the summary above, we see that the shape of the input (the output of the previous layer) is 37x37x6. The shape of the flattening layer output is 8214x1 = 37\\*37\\*6, demonstrating that the process was successful. \n"]},{"cell_type":"markdown","id":"ff41d513-52bd-4d15-bd60-823254460e77","metadata":{},"outputs":[],"source":["### Dense Layers\n"]},{"cell_type":"markdown","id":"253544db-6fd2-46b2-bc00-54d86406ffea","metadata":{},"outputs":[],"source":["Before we start making predictions, there's one last layer(s) in the neural network - dense layers. The first fully connected layer essentially uses the features we learned from previous convolutional and pooling layers to make predictions for a given image. It takes a flattened 1D array as input, calculates their weighted average, and then applies an activation function on it. As a result, each output is a combination of all the input features, making this layer \"fully-connected\".\n","\n","The last dense layer must have the softmax activation function when we have more than two classes because it will output probabilities of each label class and select the one with the highest probability as the final class of the input image.\n"]},{"cell_type":"markdown","id":"a80d5be6-2310-4724-b053-6740ee985f50","metadata":{},"outputs":[],"source":["Let's add two dense layers: one to connect with the flattened layer and the other to prepare the tensor for classification. Note that `units=5` for the second one because we have 5 categories of flowers total.\n"]},{"cell_type":"code","id":"2772778e-0256-4815-a61f-7f47279f778c","metadata":{},"outputs":[],"source":["classifier.add(Dense(units = 512, activation = 'relu'))\n\nclassifier.add(Dense(units = 5, activation = 'softmax'))"]},{"cell_type":"code","id":"170d0393-b666-42c3-9185-1cf154c8222a","metadata":{},"outputs":[],"source":["classifier.summary()"]},{"cell_type":"markdown","id":"43dde08e-9673-4066-87c6-0a38cc480b9e","metadata":{},"outputs":[],"source":["That wraps up this lab! Knowing all the components of a convolutional neural network, you're now ready to use it for advanced applications - classification, object detection, and more. We'll specifically go into how to train your model for a classification task in the next lab.\n"]},{"cell_type":"markdown","id":"b0bf094f-b928-4240-9edd-34830019930f","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"de78116a-5d1d-408d-88d1-d347bd99c28c","metadata":{},"outputs":[],"source":["[Cindy Huang](https://www.linkedin.com/in/cindy-shih-ting-huang/) is an incoming 4th year undergraduate at the University of Toronto studying data science. She has a passion for using machine learning to improve user experience.\n"]},{"cell_type":"markdown","id":"051fd9a9-b7b9-4536-8508-340facb7e96c","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"eb02514c-7a64-4d19-ad0b-b67e7b2ca36c","metadata":{},"outputs":[],"source":["[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"9f825eb3-1ed3-4a6f-a2d6-7a860fb5a25f","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"6f33cb54-ada7-42fd-9770-627d29d5cf7a","metadata":{},"outputs":[],"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2022-06-13|0.1|Cindy|Create first draft|\n","|2022-06-29|0.2|Joseph|Edited lab||2022-09-07|0.2|Steve Hord|QA pass edits|\n"]},{"cell_type":"markdown","id":"3d30e0f3-55e5-47fb-bc97-4e4ae2ea2642","metadata":{},"outputs":[],"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"80f7fd8e1374b4cb576e81fd93aa2ee86c7abbe3c0de8fe1714da8b45be1eca3"},"nbformat":4,"nbformat_minor":4}