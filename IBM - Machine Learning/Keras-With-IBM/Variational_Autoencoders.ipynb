{"cells":[{"cell_type":"markdown","id":"3bd43de2-0d1b-4f13-9491-722e404859a3","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n","</center>\n"]},{"cell_type":"markdown","id":"baf8a3d1-3b33-4b6d-8573-5b37dd91dc7b","metadata":{},"outputs":[],"source":["# Variational Autoencoders\n"]},{"cell_type":"markdown","id":"105a89f9-544a-4501-8812-69c4b719fd02","metadata":{},"outputs":[],"source":["Estimated time needed: **25** minutes\n"]},{"cell_type":"markdown","id":"8d95a260-1ba8-4e3a-b85f-2543fb43e9a6","metadata":{},"outputs":[],"source":["Variational Autoencoders are a type of deep learning generative model. Once you train them on sufficiently large datasets and let them learn latent representations of the data, they can be used to draw faces, plot digits, produce music, and generate anything you can think of.\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/vae_intro.jpg\" width=60%>\n"]},{"cell_type":"markdown","id":"172ccf2a-98e4-49a4-bfe8-f4a7aa4bed8d","metadata":{},"outputs":[],"source":["In this lab, we will study the architecture of VAEs. We will build a VAE ourselves using Keras and train the model on the MNIST digits dataset, so that they can be used to generate new images of digits.\n"]},{"cell_type":"markdown","id":"ae23ee74-380c-4330-9bff-d84fe0f59b7c","metadata":{},"outputs":[],"source":["## **Table of Contents**\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"https://#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"https://#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"https://#Defining-Helper-Functions\">Defining HelperFunctions<a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"https://#Dataset\">Prepare Dataset</a>\n","    </li>  \n","    <li>\n","        <a href=\"https://#Variational-Autoencoder\">Variational Autoencoder</a>\n","    </li>\n","        <li>\n","        <a href=\"https://#Encoder Part\">Encoder Part</a>\n","    </li>\n","     <li>\n","        <a href=\"https://#Decoder Part\">Decoder Part</a>\n","    </li>\n","         <li>\n","        <a href=\"https://#Loss-function\">Loss function </a>\n","    </li>\n","      <li>\n","        <a href=\"https://#Putting-it all together\">Putting it all together  </a>\n","    </li>\n","    <li><a href=\"https://#Training the VAE\">Training the VAE</a></li>\n","\n","</ol>\n"]},{"cell_type":"markdown","id":"48afec55-670d-472c-96bf-356f753f4865","metadata":{},"outputs":[],"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Understand the architecture of a Variational Autoencoder\n","*   Build and train Variational Autoencoder in Keras\n"]},{"cell_type":"markdown","id":"6f7843ea-6f06-49ed-963a-0e1206b9b941","metadata":{},"outputs":[],"source":["***\n"]},{"cell_type":"markdown","id":"c24b41ed-118d-4020-b970-cabf4137b76f","metadata":{},"outputs":[],"source":["## Setup\n"]},{"cell_type":"markdown","id":"598a2192-addd-4901-9f32-9b5cb7ad3d85","metadata":{},"outputs":[],"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","id":"2f5ca7c7-5915-48e3-85d6-916e641bc984","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","id":"28f8ab99-a106-4dfd-a2b9-e6c085528a23","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"markdown","id":"bc093db1-0e7e-406e-9c01-cefb30e8149e","metadata":{},"outputs":[],"source":["Run the following upgrade and then **RESTART YOUR KERNEL**. Make sure the version of tensorflow imported below is **no less than 2.9.0**.\n"]},{"cell_type":"code","id":"80d8cdae-6c1d-44f7-a911-59a428d1e7b8","metadata":{},"outputs":[],"source":["%%capture\n!pip3 install --upgrade tensorflow"]},{"cell_type":"markdown","id":"38481da1-a2d2-41f6-851c-346f70a1b2db","metadata":{},"outputs":[],"source":["### Importing Required Libraries\n","\n","*We recommend you import all required libraries in one place (here):*\n"]},{"cell_type":"code","id":"786826df-f4f0-4745-babe-d530fe13becc","metadata":{},"outputs":[],"source":["# You can use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')\n\nimport os\nimport numpy as np\n\n# Import the keras library\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow import keras\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Layer,Reshape,Conv2DTranspose\nfrom tensorflow.python.client import device_lib\nfrom keras.layers import Multiply, Add\nfrom keras import backend as K\n\nfrom numpy import random\n\nfrom matplotlib import pyplot as plt"]},{"cell_type":"markdown","id":"4adc4522-e613-4bad-b09f-b9453f8cbb9c","metadata":{},"outputs":[],"source":["### Defining Helper Functions\n"]},{"cell_type":"code","id":"16fecd5f-5935-4cd6-883c-bb3ccb785ee1","metadata":{},"outputs":[],"source":["def plot_label_clusters(model, data, labels):\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ =encoder.predict(data)\n    plt.figure(figsize=(8, 6))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.show()"]},{"cell_type":"markdown","id":"b7c14c90-75cf-4792-986a-37e3d40c959f","metadata":{},"outputs":[],"source":["## Preparing the Dataset\n"]},{"cell_type":"markdown","id":"adf796cb-b260-4594-805e-8cc8ec68300b","metadata":{},"outputs":[],"source":["We load the MNIST handwritten digit dataset:\n"]},{"cell_type":"code","id":"a68fbbc5-861d-46cf-ac08-cda7e6eba298","metadata":{},"outputs":[],"source":["# Import data\n(X_train, y_train), (_, _) = keras.datasets.mnist.load_data()"]},{"cell_type":"markdown","id":"7dba1690-f179-419e-bb8c-141cdbea3bd1","metadata":{},"outputs":[],"source":["There are 60000 $28 \\times 28$ images in the training set:\n"]},{"cell_type":"code","id":"fb002833-0de9-4cdc-9f3b-0b916866f993","metadata":{},"outputs":[],"source":["X_train.shape, y_train.shape"]},{"cell_type":"markdown","id":"4f79c93b-ac25-434a-84a5-695d32fc898a","metadata":{},"outputs":[],"source":["Let's look at the unique labels of the digits that we want to predict:\n"]},{"cell_type":"code","id":"7b5deadf-e622-4650-af00-18c2b28cb0b3","metadata":{},"outputs":[],"source":["np.unique(y_train)"]},{"cell_type":"markdown","id":"c9d950fa-0702-45c3-b67c-5803c334ce4d","metadata":{},"outputs":[],"source":["We will reshape the training set to $60000 \\times 28 \\times 28 \\times 1$ to work with convolutions. 1 indicates that the input images only have one channel, that is: grayscale images.\n"]},{"cell_type":"code","id":"66bf78cf-019e-408c-aef2-9ca7ac6d1baa","metadata":{},"outputs":[],"source":["print(f\"Before reshaping, X_train has a shape of: {X_train.shape}\")\n\nX_train = X_train.reshape((X_train.shape[0],X_train.shape[1],X_train.shape[2],1))\nprint(f\"After reshaping, X_train has a shape of: {X_train.shape}\")"]},{"cell_type":"markdown","id":"3735283c-b937-4670-bd59-b0c82ff3ab1e","metadata":{},"outputs":[],"source":["We cast the data type of `X_train` to `tf.float32` and normalize its values to range from 0 to 1:\n"]},{"cell_type":"code","id":"2b679d93-ed9d-40a0-b7ca-d9d05b20db05","metadata":{},"outputs":[],"source":["X_train = tf.cast(X_train, tf.float32)\nX_train = X_train/255.0"]},{"cell_type":"markdown","id":"eea595c8-3110-4979-b60b-edeebe9b1be2","metadata":{},"outputs":[],"source":["We convert the tensors to a `tf.data.Dataset` object.\n"]},{"cell_type":"code","id":"3e1807d2-82b2-4cd7-96df-e974a1ec3760","metadata":{},"outputs":[],"source":["dataset=tf.data.Dataset.from_tensor_slices(X_train)\ndataset"]},{"cell_type":"markdown","id":"8a1e1661-7dc7-44fe-81a4-01b71095b166","metadata":{},"outputs":[],"source":["We can plot five random samples from the training set:\n"]},{"cell_type":"code","id":"20cb91d4-9013-40c2-a2bf-5b69c9f73449","metadata":{},"outputs":[],"source":["for r in random.randint(0, 59999, size=5, dtype=int):\n    \n    plt.figure(figsize=(3,3))\n    plt.imshow(X_train[r,:,:,0],cmap=\"gray\")\n    plt.title(\"sample No.{} belongs to class {}\".format(r,y_train[r]))\n    plt.axis(\"off\")"]},{"cell_type":"markdown","id":"3f8bdd52-908b-4674-8036-141425b12cce","metadata":{},"outputs":[],"source":["## Variational Autoencoder\n","\n","In a nutshell, the architecture of a VAE is similar to that of a standard Autoencoder such that it consists of an encoder and a decoder, both of which are trained to minimize the reconstruction error between the original data $X$ and the encoded-decoded data $\\hat X$.\n","\n","The training samples $X$ are passed to the encoder to generate samples $\\boldsymbol z$, which are mappings of $X$ in the latent space.  The decoder then uses $\\boldsymbol z$ to generate the most likely reconstruction $\\hat X$.\n","\n","At this point, a natural question that comes in mind is, how do we use VAEs to generate meaningful content? You might think that if we train a VAE on images, then we can use it to generate new images. However, since it's difficult to regularize what happens to our encoder output in the latent space, we can't be sure that the encoder will organize information in the latent space in a way that the decoder can easily take that information and generates content that seems reasonable.\n","\n","Hence, in order to be able to use the VAE for meaningful generative purposes, we need to introduce some regularization into the latent space. As shown in the diagram below, **instead of mapping the input to a single point in the latent space, we encode it as a normal distribution by returning the mean and standard deviation of the distribution**. By doing so, the decoder would be able to use the regularized information to construct new content.\n","\n","In the following sections, we will build a VAE by following the architecture in the illustrative diagram below.\n"]},{"cell_type":"markdown","id":"46e41322-24de-4f12-ad73-f898fa86ff74","metadata":{},"outputs":[],"source":[" <center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/autoencoder.png\"\" width=\"800\" alt=\"computer components\"  />\n","<center>\n"]},{"cell_type":"markdown","id":"31747b08-b8a8-478b-a14f-fbc9e85cf47c","metadata":{},"outputs":[],"source":["## Encoder Part\n","\n","We build the encoder part by breaking it up into three smaller parts.\n","\n","The **first part** of the encoder consists of several fully connected layers, as shown in the picture below, which encodes the high-dimensional input; For our implementation, we will use two **convolution layers** since we want the model to learn the image data using convolutions.\n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/encoder.png\" width=\"30%\" alt=\"computer components\"/></center>\n"]},{"cell_type":"markdown","id":"bc93f0b2-656c-4ec9-b871-1098058af273","metadata":{},"outputs":[],"source":["Here is the code for building the first part of the encoder. The `encoder_output` that comes out of the convolution layers is denoted as $\\boldsymbol z^2$.\n"]},{"cell_type":"code","id":"beb635d9-ab2b-4953-ae12-c916bbc34ac7","metadata":{},"outputs":[],"source":["encoder_input= keras.Input(shape=(28, 28, 1))\nx = Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_input)\nx = Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = Flatten()(x)\nencoder_output = Dense(16, activation=\"relu\")(x)"]},{"cell_type":"markdown","id":"ea5c0d67-fd0b-4e51-919a-ad3dee62eee2","metadata":{},"outputs":[],"source":["The **second part** of the encoder represents a normal distribution over the latent space that takes the `encoder_output` and gives you the probability of it belonging to the distribution.\n","\n","The mean and standard deviation of the normal distribution will be learned and then used to calculate the log-likelihood for optimization purposes.\n"]},{"cell_type":"markdown","id":"7278cfdb-0728-44f1-84eb-a1a6d5b45041","metadata":{},"outputs":[],"source":[" <center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/mean_var.png\" width=\"400\" alt=\"computer components\"  />\n","<center>\n"]},{"cell_type":"markdown","id":"a9c722cd-3ee6-4459-958d-5182a4a88613","metadata":{},"outputs":[],"source":["To implement the second part, we create **two Dense layers** in parallel for the model to learn the **mean** and **log variance** respectively; We want the model to learn the log variance instead of the variance because it brings more stability and ease of training. The detailed reason is as follows:\n","\n","*   By definition, $\\sigma$ is a non-negative real number. To enforce this, we would need to use the ReLU activation to obtain such a value, but the gradient is not well defined around zero.\n","*   Besides, since we typically apply normalization methods in model training, the data values range from 0 to 1, which means the standard deviation of those values is also very small. This adds a burden to the optimization process and causes numerical instabilities as the gradients flowing in the backpropagation will contain floating points.\n","\n","Note that we can convert the log variance to the standard deviation using the exponential function. Here is the code for building the second part:\n"]},{"cell_type":"code","id":"877db953-1ea5-43ee-989f-743c92312469","metadata":{},"outputs":[],"source":["latent_dim = 2\n\n# Dense layer to learn the mean\nmean = Dense(latent_dim, name=\"mean\")(encoder_output)\n# Dense layer to learn the log variance\nlog_var = Dense(latent_dim, name=\"z_log_var\")(encoder_output)\n\n# sigmia is calculated from log variance\nsigma = tf.exp(0.5 * log_var)"]},{"cell_type":"markdown","id":"4862c356-edba-4da0-852d-983eeb3feaba","metadata":{},"outputs":[],"source":["Here comes the **third part** of the encoder, where we sample a point from the learned distribution in the latent space. The sampled point will later be decoded by the decoder to generate new content.\n","\n","The following diagram illustrates the random sampling:\n","\n","<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/repramateriztion.png\" width=\"400\" alt=\"computer components\"  />\n","<center> \n"]},{"cell_type":"markdown","id":"e21a47c3-a2e2-4a22-9f09-b264f917a4e9","metadata":{},"outputs":[],"source":["The sampled point, denoted by $\\boldsymbol z^5$ in the diagram, comes from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$, with some random noise $\\epsilon$.\n"]},{"cell_type":"code","id":"774110fb-40e1-4abd-a3b8-76815adf959c","metadata":{},"outputs":[],"source":["# random normal noise \nepsilon = K.random_normal(shape = (tf.shape(mean)[0], tf.shape(mean)[1]))"]},{"cell_type":"markdown","id":"3f51951c-ff65-4d7b-9d56-b5143470b80c","metadata":{},"outputs":[],"source":["Due to the random sampling of $\\boldsymbol z^5$, we use the [Reparameterization Trick](https://gregorygundersen.com/blog/2018/04/29/reparameterization/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01) $ z = \\mu + \\sigma \\odot \\epsilon$, which allows the VAE to backpropagate through a random node.\n"]},{"cell_type":"code","id":"119527b5-68db-4796-83b9-1b3f64fdc20b","metadata":{},"outputs":[],"source":["#z = mean + sigma * epsilon \n \nz_eps = Multiply()([sigma, epsilon])\nz = Add()([mean, z_eps])"]},{"cell_type":"markdown","id":"ba1b2a98-691b-43b6-8d7b-69fdd4608e99","metadata":{},"outputs":[],"source":["The encoder will output the `mean` and `log_var` of the learned distribution as well as the sampled point `z`. Now, we create the complete encoder network by chaining the `encoder_input` and the `outputs`:\n"]},{"cell_type":"code","id":"cfd2dc00-c568-4bac-8fa2-83e62487a68b","metadata":{},"outputs":[],"source":["encoder = Model(encoder_input, outputs = [mean, log_var, z], name = 'encoder')\nencoder.summary()"]},{"cell_type":"markdown","id":"d034e021-03ba-4e46-9c5c-3d10c8536847","metadata":{},"outputs":[],"source":["## Decoder Part\n","\n","The decoder part of a VAE is the same as that of a regular Autoencoder. To upsample the output from the encoder, we can use the Keras Sequential model API to group a stack of Dense layers and Transpose Convolution layers together.\n","\n","<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/decoder.png\" width=\"400\" alt=\"computer components\"  />\n","<center>\n"]},{"cell_type":"code","id":"c3f011a8-4edf-4997-b826-58e66e533b8b","metadata":{},"outputs":[],"source":["latent_dim=2\ndecoder=Sequential()\n\ndecoder.add(keras.Input(shape=(latent_dim,))) # input dimension is 2\ndecoder.add(Dense(7 * 7 * 64, activation=\"relu\"))\ndecoder.add(Reshape((7, 7, 64)))\ndecoder.add(Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\"))\ndecoder.add(Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\"))\ndecoder.add(Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\"))\ndecoder.summary()"]},{"cell_type":"markdown","id":"b84d9457-49a8-4bcd-8676-3996ff2fde4e","metadata":{},"outputs":[],"source":["## Loss Function\n"]},{"cell_type":"markdown","id":"110c4b5a-56d5-4912-8b26-8e12b24cb004","metadata":{},"outputs":[],"source":["We include the **reconstruction loss**, which compares the output of the VAE to the original input. In this case, we use the mean square error. We also include a **regularization term**, which regularizes the organization of the latent space by making the distribution learned by the encoder close to a standard normal distribution. The regularization is enforced using the Kullback–Leibler divergence that compares two distributions.\n"]},{"cell_type":"code","id":"067da211-a02c-4317-853d-fcaf9542bc67","metadata":{},"outputs":[],"source":["# make loss function \n\n#reconstruction_loss\ndef reconstruction_loss(y, y_hat):\n    return tf.reduce_mean(tf.square(y - y_hat))\n\n\n#Kullback–Leibler divergence encoder loss\ndef kl_loss(mu, log_var):\n    loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))\n    return loss\n\n# add two losses \ndef vae_loss(y_true, y_hat, mu, log_var):\n    return reconstruction_loss(y_true, y_hat) + (1 / (64*64)) * kl_loss(mean, log_var)\n"]},{"cell_type":"markdown","id":"f0a1e2be-5d16-497d-b5c5-df25567c5690","metadata":{},"outputs":[],"source":["## Putting it all Together\n"]},{"cell_type":"markdown","id":"d0ff94ad-4ae0-4917-bd1d-d22c7a485764","metadata":{},"outputs":[],"source":["We combine the Encoder and Decoder to create a VAE, and append the regularization term to the model:\n"]},{"cell_type":"code","id":"abe7e78c-8d2a-493a-aa9b-077253d3faae","metadata":{},"outputs":[],"source":["# encoder returns mean and log variance of the normal distribution,\n# and a sample point z\nmean, log_var, z = encoder(encoder_input)\n\n# decoder decodes the sample z \nreconstructed = decoder(z)\n\nmodel = Model(encoder_input, reconstructed, name =\"vae\")\nloss = kl_loss(mean, log_var)\nmodel.add_loss(loss)\nmodel.summary()"]},{"cell_type":"markdown","id":"b35378b1-350c-4e2e-9ba2-6fa5c26fbf63","metadata":{},"outputs":[],"source":["## Training the VAE\n","\n","We now train the model. After each epoch, we input random noise <code>z</code> into the decoder. We will see for each epoch, the decoder output will look more and more like a digit.\n","\n","We also output the values for the latent space z for all the different digits in our dataset, color coded according to class. We see that for each iteration, the samples appeared more clustered.\n"]},{"cell_type":"code","id":"70dace1c-04f3-452c-9105-b34a77df8a84","metadata":{},"outputs":[],"source":["#loss\nmse_losses = []\nkl_losses = []\n# optimizer \noptimizer =  tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5, beta_2 = 0.999 )\nepochs = 5\n\nfor epoch in range(epochs):\n    \n    print(f\"Samples generated by decoder after {epoch} epoch(s): \")\n    \n    #random noise\n    z = tf.random.normal(shape = (5, latent_dim,))\n\n    # input random noise into the decoder\n    xhat = decoder.predict(z)\n    \n    # plot the decoder output\n    plt.figure()\n    for i in range(5):\n        plt.subplot(1,5,i+1)\n        plt.imshow(xhat[i,:,:,0],cmap=\"gray\")\n        plt.axis(\"off\")\n    plt.show()\n\n    print(f\"2D latent representations of the training data produced by encoder after {epoch} epoch(s): \")\n    plot_label_clusters(encoder, X_train, y_train)\n\n\n    # training steps\n    for (step, training_batch) in enumerate(dataset.batch(100)):\n        with tf.GradientTape() as tape:\n\n            # model output\n            reconstructed = model(training_batch)\n\n            y_true = tf.reshape(training_batch, shape = [-1])\n            y_pred = tf.reshape(reconstructed, shape = [-1])\n\n            # calculate reconstruction loss\n            mse_loss = reconstruction_loss(y_true, y_pred)\n            # calculate KL divergence\n            kl = sum(model.losses)\n\n            kl_losses.append(kl.numpy())\n            mse_losses.append(mse_loss .numpy())\n\n            # total loss\n            train_loss = 0.01 * kl + mse_loss\n\n            grads = tape.gradient(train_loss, model.trainable_variables)\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n         \n    print(\"Epoch: %s - Step: %s - MSE loss: %s - KL loss: %s\" % (epoch, step, mse_loss.numpy(), kl.numpy()))\n  "]},{"cell_type":"markdown","id":"241e34b5-3542-45ad-8a6a-1d4c400578b8","metadata":{},"outputs":[],"source":["We plot the reconstruction loss and Kullback–Leibler divergence against the number of training iterations, we can see that they are both decreasing.\n"]},{"cell_type":"code","id":"2bdbe889-a48f-4cfd-8fce-2738baba965d","metadata":{},"outputs":[],"source":["plt.plot(mse_losses)\nplt.title(\" reconstruction loss \")\nplt.show()\nplt.plot(kl_losses)\nplt.title(\"  Kullback–Leibler divergence\")\nplt.show()"]},{"cell_type":"markdown","id":"4780e779-b906-4d2a-a0d4-aae036cc42d0","metadata":{},"outputs":[],"source":["Now that our VAE has been trained, we can use its decoder network to generate some samples. Let's see if our decoder can do a good job in generating artificial images that look like digits!\n"]},{"cell_type":"code","id":"269cf7b5-c018-40ed-926d-99e477c3dc44","metadata":{},"outputs":[],"source":["xhat = decoder.predict(z)\n\nfor i in range(5):\n    plt.imshow(xhat[i,:,:,0],cmap=\"gray\")\n    plt.show()"]},{"cell_type":"markdown","id":"72b8e4ff-3f29-4f95-adf5-4803de0588f7","metadata":{},"outputs":[],"source":["We can also visualize the final latent representations of the training data produced by the trained encoder on a 2D plot, colored by classes.\n"]},{"cell_type":"code","id":"5be02030-6998-4135-a9f8-6a93c8f0a5a7","metadata":{},"outputs":[],"source":["plot_label_clusters(encoder, X_train, y_train)"]},{"cell_type":"markdown","id":"2c2b8b71-30f4-43c8-8ac3-7a34fe471717","metadata":{},"outputs":[],"source":["## Congratulations! You have completed this lab.\n"]},{"cell_type":"markdown","id":"46bd6d48-9415-48d1-ba59-93cdcc1e55a5","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"8ef5302f-ffa6-494d-b2c1-c29da21cd914","metadata":{},"outputs":[],"source":["[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01) has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n","\n","[Roxanne Li](https://www.linkedin.com/in/roxanne-li/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01) is a Data Science intern at IBM Skills Network, entering level-5 study in the Mathematics & Statistics undergraduate Coop program at McMaster University.\n"]},{"cell_type":"markdown","id":"157485b9-243a-4b6c-b641-e7f81d1e7ab0","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"c9eacc5b-1e3e-41e0-ab5f-784efa08fc2d","metadata":{},"outputs":[],"source":["| Date (YYYY-MM-DD) | Version | Changed By | Change Description      |\n","| ----------------- | ------- | ---------- | ----------------------- |\n","| 2022-09-14        | 0.1     | Joseph S.  | Created Lab             |\n","| 2022-09-19        | 0.1     | Roxanne Li | Reviewed and edited lab |\n"]},{"cell_type":"markdown","id":"7c148313-a18c-4490-b70a-184c0975ba74","metadata":{},"outputs":[],"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"language_info":{"name":""},"kernelspec":{"name":"","display_name":""}},"nbformat":4,"nbformat_minor":4}