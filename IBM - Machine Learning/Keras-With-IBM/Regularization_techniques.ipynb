{"cells":[{"cell_type":"markdown","id":"08fbc364-b563-4abd-9b54-878424c862b0","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"3680d26d-6403-4483-ac40-6fadba5316f9","metadata":{},"outputs":[],"source":["# **Regularization Techniques**\n"]},{"cell_type":"markdown","id":"ba954de6-113a-4be2-98ef-b2989c3f902f","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"3b945596-eac0-4099-b157-ca08b0052b11","metadata":{},"outputs":[],"source":["In this lab, you will learn five regularization techniques that are commonly used in training neural networks. You will see how they are implemented using Keras and the result of applying each of them on a real world dataset.\n"]},{"cell_type":"markdown","id":"2c1e2d69-1788-4be7-9e50-486cae3c8856","metadata":{},"outputs":[],"source":["## Table of Contents\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>     \n","    </li>\n","    <li><a href=\"https://#Overfitting\">Overfitting</a></li>\n","    <li><a href=\"https://#L2-Regularization\">L2 Regularization</a></li> \n","    <li><a href=\"https://#L1-Regularization\">L1 Regularization</a></li>\n","    <li><a href=\"https://#Dropout\">Dropout</a></li> \n","    <li><a href=\"https://#Batch-Normalization\">Batch Normalization</a></li>\n","    <li><a href=\"#Example 1: Apply regularization techniques on simulated data\">Example 1: Apply regularization techniques on simulated data</a></li>\n","    <li><a href=\"#Example 2: Spam Classification\">Example 2: Spam Classification</a></li>\n","</ol>\n"]},{"cell_type":"markdown","id":"7c7013a3-6244-4d4d-8d0d-63ff7cb10324","metadata":{},"outputs":[],"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Understand how the regularization techniques (L1, L2, Dropout, BatchNorm) work for neural networks.\n","*   Describe the difference between L1 and L2 regularization.\n","*   Apply the four types of regularization when training neural networks.\n"]},{"cell_type":"markdown","id":"a8964727-92b5-4137-bec3-4035e142574d","metadata":{},"outputs":[],"source":["***\n"]},{"cell_type":"markdown","id":"ccb84580-5dfb-4c08-af47-973139aa195e","metadata":{},"outputs":[],"source":["## Setup\n"]},{"cell_type":"markdown","id":"c9febd70-3158-4aa3-b375-d92992b0e0fc","metadata":{},"outputs":[],"source":["For this lab, we will be using the following libraries:\n","\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`Pillow`](https://pillow.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for image processing functions.\n","*   [`OpenCV`](https://docs.opencv.org/4.x/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for other image processing functions.\n","*   [`tensorflow`](https://www.tensorflow.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and neural network related functions.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","id":"75f71ecb-64fb-4af4-9201-de198062c1ef","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Anaconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the following code cell.\n"]},{"cell_type":"code","id":"386c2f5e-e9bc-466a-8cb1-ca042ecdd071","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed, as follows. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 opencv-python==4.5.5.62\n\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n!pip install --upgrade tensorflow\n# RESTART YOUR KERNEL AFTERWARD AS WELL"]},{"cell_type":"markdown","id":"41a1b51b-7e00-4d6f-ac05-e1eb899d32ae","metadata":{},"outputs":[],"source":["You will need the nltk library for this lab, make sure to install it.\n"]},{"cell_type":"code","id":"66a79935-ab3c-4ad3-94b0-7d44d803fcec","metadata":{},"outputs":[],"source":["%%capture\n!pip install nltk"]},{"cell_type":"markdown","id":"84655757-d7f3-4fda-a3c8-42ab81e097cd","metadata":{},"outputs":[],"source":["### Importing Required Libraries\n","\n","*We recommend you import all required libraries in one place, as follows:*\n"]},{"cell_type":"code","id":"4020a430-d29c-484a-8491-c78ddb67df49","metadata":{},"outputs":[],"source":["import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense"]},{"cell_type":"markdown","id":"22012c87-4a49-42ec-b579-e2a4c5a639a4","metadata":{},"outputs":[],"source":["### Defining Helper Functions\n"]},{"cell_type":"markdown","id":"45b6c2fc-8837-4897-9f17-b4f8e5593e3f","metadata":{},"outputs":[],"source":["This function helps prepare the SMS Spam dataset.\n"]},{"cell_type":"code","id":"6912c6e5-dcf6-464a-806d-807032fff2aa","metadata":{},"outputs":[],"source":["def prepare_data():\n    try:\n        data = pd.read_csv(\"spam.csv\", encoding='latin-1')\n    except FileNotFoundError:\n        print(\"Data file not found, make sure it's downloaded.\")\n        \n    data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\n    data.rename(columns={\"v1\": \"label\", \"v2\": \"text\"}, inplace=True)\n    data.label = data['label'].map({'ham':0, 'spam':1})\n    data['Count'] = data['text'].apply(lambda x: len(x))\n    \n    sw=stopwords.words(\"english\")\n    vectorizer = TfidfVectorizer(stop_words=sw, binary=True)\n\n    X = vectorizer.fit_transform(data.text).toarray()\n    y = data.label\n    \n    return X, y"]},{"cell_type":"markdown","id":"5d4de98b-21a6-41da-b891-8583229d943a","metadata":{},"outputs":[],"source":["This function plots the loss and accuracy curve from the training history of a neural network.\n"]},{"cell_type":"code","id":"d86a7252-db17-4eaf-b79d-ace44bdc9a04","metadata":{},"outputs":[],"source":["def plot_metrics(history):\n    fig = plt.figure(figsize=(10,5))\n    for i, metric in enumerate(['accuracy', 'loss']):\n        train_metrics = history.history[metric]\n        val_metrics = history.history['val_'+metric]\n        epochs = range(1, len(train_metrics) + 1)\n        plt.subplot(1,2,i+1)\n        plt.plot(epochs, train_metrics)\n        plt.plot(epochs, val_metrics)\n        plt.title('Training and validation '+ metric)\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(metric)\n        plt.legend([\"train_\"+metric, 'val_'+metric])\n"]},{"cell_type":"markdown","id":"992fbbb2-e7ad-443c-8e9c-3fca78d6e9ab","metadata":{},"outputs":[],"source":["## Overfitting\n"]},{"cell_type":"markdown","id":"b54887fa-d134-4703-8efd-7747f9c44441","metadata":{},"outputs":[],"source":[" In the following figure, we see a two-class classification problem. The red points represent one class, and the blue points represent the second class. The actual decision boundary is shown in black. Most points are in the correct region, but several samples are in the incorrect region; this could be for many reasons, such as noise or outliers. The key point is that you do not expect data points of these classes in these regions.\n","\n","The second model in green is meant to approximate the decision boundary; this model classifies the data points, even the data points in the incorrect regions. This model does not generalize well. This is because most data points will not be in those regions. As a result, the data will do poorly on data not from the original dataset, like data in the real world. This is called **overfitting**.\n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L3/images/Overfitting.png\" width=\"40%\"></center>\n","\n","<p style=\"text-align:center\">\n","    <a href=\"https://en.wikipedia.org/wiki/Overfitting\">Source: Wikipedia</a>\n","</p>\n","\n","To combat overfitting or high variance, we introduce four regularization techniques in this lab for you to choose from the next time you train your neural network. They are:\n","\n","- L2 (Ridge) regularization\n","- L1 (Lasso) regularization\n","- Dropout\n","- Batch Normalization \n","- Data shuffling\n"]},{"cell_type":"markdown","id":"2b609c28-25a6-4d28-b743-e780668cbdae","metadata":{},"outputs":[],"source":["## L2 Regularization\n"]},{"cell_type":"markdown","id":"badd9973-1cb6-4a16-82ba-69f1386fda0d","metadata":{},"outputs":[],"source":["L2 regularization is perhaps the most common form of regularization analogous to the penalty term in  **Ridge regression**. It penalizes the squared magnitude of the weights $\\boldsymbol w$ by adding the term $\\frac12 \\lambda w^2$ to the objective function that the algorithm is trying to optimize. $\\lambda$ is the regularization strength, also called the **shrinkage parameter** which can be tuned during training. \n","\n","The regularization term is defined as the Euclidean norm of the weight matrices, which sums over all the squared weights. It is multiplied by $\\frac12$ so that when the gradient is calculated we get $\\lambda w$.\n","\n","As the shrinkage parameter approaches infinity, the weights are driven down to near zero but **will not be exactly zero**. \n"]},{"cell_type":"markdown","id":"9f0ae49a-ee50-430d-b04f-774e419ddbff","metadata":{},"outputs":[],"source":["### Using L2 Regularization in Keras\n","\n","In Keras, regularization penalties are applied on a per-layer basis, that is, you specify the name of the regularizer API as you add a layer to the network. The `tf.keras.regularizers` module has a built-in L2 class that you can call using the following:\n"]},{"cell_type":"code","id":"b2c9b69c-3774-4d5e-9be0-84b1afb48bcf","metadata":{},"outputs":[],"source":["tf.keras.regularizers.l2(l=0.01) "]},{"cell_type":"markdown","id":"043ced38-08a7-420f-9518-cd4098878e7b","metadata":{},"outputs":[],"source":["This object can be passed as an argument to a Dense layer:\n"]},{"cell_type":"code","id":"fa04ccbd-f955-4c3b-92be-c676a914dd06","metadata":{},"outputs":[],"source":["dense_layer = Dense(32, \n                activation=\"relu\", \n                kernel_regularizer=tf.keras.regularizers.l2(l=0.01))"]},{"cell_type":"markdown","id":"d104a36a-d9ee-4509-a662-0e7337eab466","metadata":{},"outputs":[],"source":["## L1 Regularization\n","\n","L1 regularization is another common form of regularization. It penalizes large weights $\\boldsymbol w$ by adding the term $\\lambda |w|$ to the objective function, where $\\lambda$ is the regularization strength. \n","\n","What differentiates L1 from L2 is that, when $\\lambda$ approaches infinity, L1 regularization can shrink the weights of the less important features to zero, which would be very useful if you also want automatic feature selection during training. \n","\n","<p style='color: blue'>The following figure illustrates how L1 and L2 work differently:</p>\n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L3/images/L12_1.png\" width=\"65%\"></center>\n","\n","<p style=\"text-align:center; color:gray\">Source: An Introduction to Statistical Learning</p>\n"]},{"cell_type":"markdown","id":"3dd97b41-061b-42a6-b6b2-f9aaa142e84f","metadata":{},"outputs":[],"source":["In the figure above, the red ellipses represent the contours of the loss function that needs to be optimized during training, and the green areas on the left and right represent the **feasible regions** of the L1 and L2 constaints respectively. \n","\n","In the case of L1 (left), the ellipse would be able to intersect with the feasible region on an axis, which is when the weights become zero. However, for L2 (right), due to the round shape of the feasible region; that is, the L2 norm, the ellipse wouldn't intersect with the green circle on an axis, which is why the weights regularized by L2 can be close to zero but not equal to zero.\n","\n","We can also combine the L1 regularization and the L2 regularization to obtain the **Elastic Net regularization**: $\\frac12 \\lambda_1 w^2 + \\lambda_2 |w|$. $\\lambda_1$ and $\\lambda_2$ can be tuned as hyperparameters during training.\n"]},{"cell_type":"markdown","id":"dcb8414d-7b93-4ca3-b0c2-2a81f753539a","metadata":{},"outputs":[],"source":["### Using L1 Regularization in Keras\n","\n","Similar to how we used L2, `tf.keras.regularizers` module also has a built-in L1 class:\n"]},{"cell_type":"code","id":"1be72bbc-1edf-4931-96db-ae88a3329781","metadata":{},"outputs":[],"source":["dense_layer = Dense(32, \n            activation=\"relu\", \n            kernel_regularizer=tf.keras.regularizers.l1(l=0.01))"]},{"cell_type":"markdown","id":"996fcfc8-7f66-42b8-8021-19cd3d31aa70","metadata":{},"outputs":[],"source":["If you don't need to specify a value for the regularization strength, you can also pass a string value to the `kernel_regularizer` argument:\n"]},{"cell_type":"code","id":"a0fae9e8-d167-4200-9799-e10b14c9f84f","metadata":{},"outputs":[],"source":["dense_layer = Dense(32, \n                activation=\"relu\", \n                kernel_regularizer=\"l1\")"]},{"cell_type":"markdown","id":"e9ca1393-f8a3-41e0-9dfd-085dee6c5971","metadata":{},"outputs":[],"source":["## Dropout\n","\n","During training, Dropout will keep a neuron active with some probability $\\boldsymbol p$ (a hyperparameter), or set it to zero otherwise. This regularization technique approximates training a large number of different neural networks in parallel.\n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L3/images/DO_1.png\" width=\"60%\"></center>\n","\n","Picture from the [Dropout paper](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n","\n","Dropout can be interpreted as sampling a neural network within the full neural network and only updating the weights of the sampled network. By doing so, each iteration is using a different model architecture and has a different \"view\" of the configured layers. This adds noise to the training process and perhaps breaks up situations where network layers co-adapt to correct prior mistakes, making the network more robust.\n","\n","Note that **Dropout is not used during prediction**.\n"]},{"cell_type":"markdown","id":"45db458f-e7b3-4c45-ae3a-0b1317bdb033","metadata":{},"outputs":[],"source":["### Using Dropout in Keras\n","\n","In Keras, Dropout can be applied using the Dropout class from **keras.layers**. A dropout rate can be specified when creating the dropout layer, which is the percentage of neurons that will be turned off during one update.\n"]},{"cell_type":"code","id":"4ce83280-9895-40ca-8651-6df7f5bcef22","metadata":{},"outputs":[],"source":["from tensorflow.keras.layers import Dropout\n\ndropout_layer = Dropout(rate=0.2)"]},{"cell_type":"markdown","id":"f3716d64-746c-4fd1-bfa7-2cf17deb417f","metadata":{},"outputs":[],"source":["## Batch Normalization\n","\n","Training deep neural networks is complicated, because the distribution of each layer's inputs changes as the weights of the previous layers get updated during back propagation. This can result in the learning algorithm always pursuing a moving target. In the [original paper](https://arxiv.org/abs/1502.03167) of Batch Normalization, the change in the distribution of inputs to layers in the network is referred to as **\"internal covariate shift\"**. This slows down the training by requiring lower learning rates and careful parameter initialization, making it hard for the model to generalize well. Sometimes, if the training samples are statistically too different from the testing samples, it is considered a covariate shift.\n"]},{"cell_type":"markdown","id":"28c786a5-199d-4649-8190-668b29c94801","metadata":{},"outputs":[],"source":["### What is Batch Normalization?\n","\n","Batch normalization is a technique for training very deep neural networks that normalizes the inputs to a layer for every mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n","\n","**Batch**, or **mini-batch**, is a collection of samples that will be passed through the network at one time for the weights update. **Normalization** is the process of transforming the data to have a mean 0 and a standard deviation 1 (thus follows the standard normal distribution). \n","\n","In a neural network, batch normalization is achieved through a normalization layer between the fully connected Dense layers, which fixes the means and variances of each layer's inputs. This way, there is not much change in the distribution of each layer input, and the layers in the network can learn from back-propagation simultaneously without having to wait for the previous layers to learn. This accelerates the training process.\n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L3/images/BN_6.png\" width=\"70%\"></center>\n","\n","Image credits to [Ilango Rajagopal](https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85)\n"]},{"cell_type":"markdown","id":"5577e600-702f-47fe-849d-1ac4bdf6189d","metadata":{},"outputs":[],"source":["### How does Batch Normalization work?\n","\n","Denote by $\\boldsymbol B$ a mini-batch of size $\\boldsymbol m$ of the training set, for a layer with **_d_**-dimensional input $x = (x^{(1)},...,x^{(d)})$, each dimension of its input is then normalized as:\n","$$\n","\\hat{x}_{i}^{k} = \\frac {x_i ^{k}-\\mu_B^{k}} {\\sqrt{{\\sigma_B^{k}}^2 +\\epsilon}}\n","$$\n","\n","where $k \\in [1,d]$and $i \\in [1,m]$; $\\mu_B^{k}$ and ${\\sigma_B^{k}}^2$are the **per dimension mean and variance** of $\\boldsymbol B$ such that:\n"]},{"cell_type":"markdown","id":"e3cb1588-8405-4233-ab07-d1490983589b","metadata":{},"outputs":[],"source":["$$\n","\\mu_B^{k} = \\frac{1}{m} \\sum_{i=1}^m x_i^{k}, {\\sigma_B^{k}}^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i^{k}-\\mu_B^{k})^2\n","$$\n"]},{"cell_type":"markdown","id":"ce26aec2-ebe4-40a7-8bf8-46937835e4de","metadata":{},"outputs":[],"source":["$\\epsilon$ is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized activation $\\hat{x}^{k}$ has zero mean and unit variance if $\\epsilon$ is not taken into account. To restore the representation power of the network, a transformation step then follows as\n","\n","$$\n","y_i^{k} = \\gamma^{k} \\hat{x}_{i}^{k} +\\beta^{k}\n","$$\n","\n","where the parameters $\\gamma^{k}$ and $\\beta^{k}$ are subsequently learned in the optimization process. You can refer to [wikipedia](https://en.wikipedia.org/wiki/Batch_normalization) for more mathematical details of batch normalization.\n"]},{"cell_type":"markdown","id":"447fd3b5-d423-4903-80b7-3a028f20ba58","metadata":{},"outputs":[],"source":["<p style='color: blue'>What are the benefits of Batch Normalization?</p>\n","\n","1. It tackles the internal covariate shift issue by always normalizing the input signals, thus accelerating the training of deep neural nets and increasing the generalization power of the networks.\n","2. It makes the optimization landscape smoother, reducing the magnitude of variations in the gradient and the loss, making the training faster and easier.\n","3. It acts as a regularizer by introducing random noises, as each minibatch at each layer is normalized using its own mean and standard deviation during training.\n"]},{"cell_type":"markdown","id":"bc49efc5-8041-4b62-a040-39646c0a600f","metadata":{},"outputs":[],"source":["### Using Batch Normalization in Keras\n","\n","In Keras, you can add a Batch Norm layer between the Dense layers by calling `BatchNormalization()` from **keras.layers**. Here is a code example:\n"]},{"cell_type":"code","id":"6e189c5b-1fc5-4a33-99a7-1739e386e000","metadata":{},"outputs":[],"source":["from tensorflow.keras.layers import Dense, BatchNormalization\n\nbatchnorm_layer = BatchNormalization()"]},{"cell_type":"markdown","id":"1d2c557a-c41b-4d0c-a721-ac52d0c1165b","metadata":{},"outputs":[],"source":["## Example 1: Apply regularization techniques on simulated data\n","\n","In this example, we will use the **generate_data** function to simulate a degree-3 polynomial dataset with some random noise. We will intentionally overfit the data by creating polynomial features of the data, and then we will observe whether the four regularization techniques can mitigate overfitting in this case.\n"]},{"cell_type":"markdown","id":"75e22d14-2ae1-4bd9-839c-c47b30d1a019","metadata":{},"outputs":[],"source":["Let's first visualize the polynomial data:\n"]},{"cell_type":"code","id":"5a616645-3f81-4623-9e6d-c50cf73b04c9","metadata":{},"outputs":[],"source":["def generate_data(seed=43,std=0.1,samples=500):\n    np.random.seed(seed)\n    X =np.linspace(-1,1,samples)\n    f = X**3 +2*X**2 -X \n    y=f+np.random.randn(samples)*std\n    \n    return X, y\n\n\nX,y = generate_data()\nf = X**3 +2*X**2 -X\nplt.plot(X, y,'rx',label=\"data samples\")\nplt.plot(X, f,label=\"true function\")\nplt.title(\"data and true function\")\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"6fda5daf-22c3-4a8c-9b71-3b97f3e606cf","metadata":{},"outputs":[],"source":["let's also add some outliers to y:\n"]},{"cell_type":"code","id":"4b0b2c43-0bcf-4332-bf89-036e2d7f5ed6","metadata":{},"outputs":[],"source":["y[20:30] = 0\ny[100:110] = 2\ny[180:190] = 4\ny[260:270] = -2\ny[340:350] = -3\ny[420:430] = 4\n\nplt.plot(X, y,'rx',label=\"data samples\")\nplt.plot(X, f,label=\"true function\")\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"dd720429-c631-4fec-be91-ad581970c3bf","metadata":{},"outputs":[],"source":["Let's use an overly complex neural network to fit our simulated data.\n"]},{"cell_type":"code","id":"3c8288a6-8b43-4a8e-beb9-42c0a3ad47fc","metadata":{},"outputs":[],"source":["from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential()\nmodel.add(Dense(1000, activation='relu',input_shape=(1,)))\nmodel.add(Dense(120,activation='relu'))\nmodel.add(Dense(120,activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(lr=1e-3), loss=\"mean_squared_error\")\nmodel.fit(X, y,  epochs=20, batch_size=100)"]},{"cell_type":"markdown","id":"f243fc8e-a285-47f4-b1b1-58878cfb27c0","metadata":{},"outputs":[],"source":["When we plot out the original data and the predictions, we can see that because the model's weights are not regularized, it tends to over fit  on new data: \n"]},{"cell_type":"code","id":"442079a7-6a75-401d-a554-ce5708ca8af9","metadata":{},"outputs":[],"source":["y_pred = model.predict(X)\nplt.plot(X, y,'rx',label=\"data samples\")\nplt.plot(X, f,label=\"true function\")\nplt.plot(X, y_pred ,label=\"predicted function\")\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"859c7e3c-3fed-4a5e-9e63-9ded85e7f8e1","metadata":{},"outputs":[],"source":["We can also calculate the mean square error:\n"]},{"cell_type":"code","id":"51f38540-c912-44b7-88ed-fd8b0ab954da","metadata":{},"outputs":[],"source":["no_reg = np.mean((y-y_pred)**2)\nprint(f\"Mean squared error is {no_reg}\\n\")"]},{"cell_type":"markdown","id":"044e38c8-c974-4f07-b8e6-69ffed58f5c8","metadata":{},"outputs":[],"source":["Now let's apply the four regularization techniques: **L1, L2, Drop out, and Batch Normalization** and see which works best for our data!\n"]},{"cell_type":"markdown","id":"69e4a523-78ea-457c-a4bf-cd65e84803b0","metadata":{},"outputs":[],"source":["### L1 (Lasso)\n"]},{"cell_type":"code","id":"264f5f2e-9973-479f-b837-47519fda9a35","metadata":{},"outputs":[],"source":["model_l1 = Sequential()\n\nmodel_l1.add(Dense(1000, activation='relu',input_shape=(1,),kernel_regularizer=keras.regularizers.l1(l=0.01)))\nmodel_l1.add(Dense(120,activation='relu',kernel_regularizer=keras.regularizers.l1(l=0.001)))\nmodel_l1.add(Dense(120,activation='relu'))\nmodel_l1.add(Dense(1))\nmodel_l1.compile(optimizer=Adam(lr=1e-3), loss=\"mean_squared_error\")\nmodel_l1.fit(X, y,  epochs=20, batch_size=100)"]},{"cell_type":"markdown","id":"c0474b0b-091c-4c48-b0fc-b67e73cc464e","metadata":{},"outputs":[],"source":["We can plot the model, we see the model tracks the data: \n"]},{"cell_type":"code","id":"efe938cc-dada-472a-8f40-05a6e5513a42","metadata":{},"outputs":[],"source":["y_pred = model_l1.predict(X)\nplt.plot(X, y,'rx',label=\"data samples\")\nplt.plot(X, f,label=\"true function\")\nplt.plot(X, y_pred,label=\"predicted function\")\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"edad0ca9-43b8-4943-afd0-4a4973b02818","metadata":{},"outputs":[],"source":["We can also calculate the mean square error; we see L1 regulation decreases the error:\n"]},{"cell_type":"code","id":"88249da4-11b6-44a8-8558-afba3477e03c","metadata":{},"outputs":[],"source":["l1 = np.mean((y-y_pred)**2)\nprint(f\"Mean squared error is {l1}\\n\")"]},{"cell_type":"markdown","id":"783a2744-b6b2-440b-91af-680062368097","metadata":{},"outputs":[],"source":["### L2 (Ridge)\n"]},{"cell_type":"code","id":"7b0bd598-b1aa-444c-b339-e7a70a913ce6","metadata":{},"outputs":[],"source":["model_l2 = Sequential()\n\nmodel_l2.add(Dense(1000, activation='relu',input_shape=(1,),kernel_regularizer=keras.regularizers.l2(l=0.0001)))\nmodel_l2.add(Dense(120,activation='relu',kernel_regularizer=keras.regularizers.l2(l=0.0001)))\nmodel_l2.add(Dense(120,activation='relu',kernel_regularizer=keras.regularizers.l2(l=0.0001)))\nmodel_l2.add(Dense(1))\nmodel_l2.compile(optimizer=Adam(lr=1e-3), loss=\"mean_squared_error\")\nmodel_l2.fit(X, y, validation_split=0.2, epochs=20, batch_size=40)"]},{"cell_type":"markdown","id":"aef49984-9705-42a3-a7a8-4637dd2db33c","metadata":{},"outputs":[],"source":["We can plot the model, we can also calculate the mean square error; we see L2 regulation can also improve the result by a little bit:\n"]},{"cell_type":"code","id":"0548309a-6d37-48f4-b996-6db6c334161a","metadata":{},"outputs":[],"source":["y_pred = model_l2.predict(X)\nplt.plot(X, y,'rx',label=\"data samples\")\nplt.plot(X, f,label=\"true function\")\nplt.plot(X, y_pred ,label=\"predicted function\")\nplt.legend()"]},{"cell_type":"code","id":"90fa51ce-a6bf-4c1f-8766-a4b2608f7873","metadata":{},"outputs":[],"source":["l2 = np.mean((y-y_pred)**2)\nprint(f\"Mean squared error is {l2}\\n\")"]},{"cell_type":"markdown","id":"7446d5a2-077e-4a33-90dc-6bc25f8a4b72","metadata":{},"outputs":[],"source":["### Dropout\n"]},{"cell_type":"code","id":"4c2678fc-9dc2-450b-ae42-b108b7535714","metadata":{},"outputs":[],"source":["model_dp = Sequential()\n\nmodel_dp.add(Dense(1000, activation='relu',input_shape=(1,)))\nmodel_dp.add(Dropout(0.1))\nmodel_dp.add(Dense(120,activation='relu'))\nmodel_dp.add(Dropout(0.1))\nmodel_dp.add(Dense(120,activation='relu'))\nmodel_dp.add(Dropout(0.1))\nmodel_dp.add(Dense(1))\nmodel_dp.compile(optimizer=Adam(lr=1e-3), loss=\"mean_squared_error\")\nmodel_dp.fit(X, y, validation_split=0.2, epochs=20, batch_size=40)"]},{"cell_type":"markdown","id":"5c34d938-b2aa-4626-b233-91e6e26ca3d7","metadata":{},"outputs":[],"source":["Let's plot the prediction and calculate the mean square error;\n"]},{"cell_type":"code","id":"2afd3ad3-b0ea-4f6f-98d7-1a46a6a2b0e4","metadata":{},"outputs":[],"source":["y_pred = model_dp.predict(X)\nplt.plot(X, y,'rx',label=\"data samples\")\nplt.plot(X, f,label=\"true function\")\nplt.plot(X, y_pred ,label=\"predicted function\")\nplt.legend()"]},{"cell_type":"code","id":"6f91ec62-4a46-4a9e-9c82-8b4f387dae08","metadata":{},"outputs":[],"source":["dp = np.mean((y-y_pred)**2)\nprint(f\"Mean squared error is {dp}\\n\")"]},{"cell_type":"markdown","id":"4586a0e1-064c-4e44-a26b-6ab576bb995e","metadata":{},"outputs":[],"source":["### Batch Norm\n"]},{"cell_type":"code","id":"15631a12-6960-4217-ad43-e719de7da438","metadata":{},"outputs":[],"source":["model_bn = Sequential()\n\nmodel_bn.add(Dense(1000, activation='relu',input_shape=(1,)))\nmodel_bn.add(BatchNormalization())\nmodel_bn.add(Dense(120,activation='relu'))\n\nmodel_bn.add(Dense(120,activation='relu'))\nmodel_bn.add(Dense(1))\nmodel_bn.compile(optimizer=Adam(lr=1e-3), loss=\"mean_squared_error\")\nmodel_bn.fit(X, y, validation_split=0.2, epochs=20, batch_size=40)"]},{"cell_type":"markdown","id":"0edae3d0-9e70-4386-b422-53208f4c6d19","metadata":{},"outputs":[],"source":["We can observe that due to a stronger regularization of Batch Normalization, the model does not seem to fit the data very well. However, the mean square error actually shows a big improvement.\n"]},{"cell_type":"code","id":"6dc2f60e-5459-47b5-bb4b-24c835d3ce59","metadata":{},"outputs":[],"source":["y_pred = model_bn.predict(X)\nplt.plot(X, y,'rx',label=\"data samples\")\nplt.plot(X, f,label=\"true function\")\nplt.plot(X, y_pred ,label=\"predicted function\")\nplt.legend()"]},{"cell_type":"code","id":"3081a0d5-b65e-4bf7-b206-e77eda423457","metadata":{},"outputs":[],"source":["bn = np.mean((y-y_pred)**2)\nprint(f\"Mean squared error is {bn}\\n\")"]},{"cell_type":"markdown","id":"a887826b-9fb1-4299-9d0b-694e59d5cf43","metadata":{},"outputs":[],"source":["## Data Shuffling\n","\n","To apply data shuffling to avoid overfitting, we can simpy call `shuffle=True` when fitting the model.\n"]},{"cell_type":"code","id":"721c49e0-5fe7-4ade-8317-b8ec37c634f3","metadata":{},"outputs":[],"source":["model_sh = Sequential()\n\nmodel_sh.add(Dense(1000, activation='relu',input_shape=(1,)))\nmodel_sh.add(Dense(120,activation='relu'))\nmodel_sh.add(Dense(120,activation='relu'))\nmodel_sh.add(Dense(1))\n\nmodel_sh.compile(optimizer=Adam(lr=1e-3), loss=\"mean_squared_error\")\nmodel_sh.fit(X, y, validation_split=0.2, epochs=20, batch_size=40,shuffle=True)"]},{"cell_type":"markdown","id":"ba92c0d9-edd7-4ba0-b4e9-a118f88b4740","metadata":{},"outputs":[],"source":["We can also calculate the mean square error on the data. Data shuffling doesn't seem to be very effective for our data.\n"]},{"cell_type":"code","id":"37c0691f-b430-4421-8fd5-4b384210912c","metadata":{},"outputs":[],"source":["y_pred = model_sh.predict(X)\n\nplt.plot(X, y,'rx',label=\"data samples\")\nplt.plot(X, f,label=\"true function\")\nplt.plot(X, y_pred ,label=\"predicted function\")\nplt.legend()"]},{"cell_type":"code","id":"a0559ecb-61fc-40b3-ba2c-c7df891310ec","metadata":{},"outputs":[],"source":["sh = np.mean((y-y_pred)**2)\nprint(f\"Mean squared error is {sh}\\n\")"]},{"cell_type":"markdown","id":"eace5d6f-ef28-46d7-98d4-cef0b7146501","metadata":{},"outputs":[],"source":["Let's compare the mean squared errors of the networks trained using different regularization techniques:\n"]},{"cell_type":"code","id":"36296bc8-b5ba-46c4-ae11-ddd86297a6a5","metadata":{},"outputs":[],"source":["names = ['No_reg','L1','L2','Drop_out','Batch_norm','Data_shuffling']\nerror = [no_reg, l1, l2, dp, bn, sh]\n\nplt.figure(figsize=(8, 4))\nplt.bar(names, error, width=0.6)\nplt.title(\"Mean Squared Error\", fontsize=13)\n\nfor i, err in enumerate(error):\n    plt.text(i-0.2, err+0.1, str(round(err,3)), color='blue', va='center') "]},{"cell_type":"markdown","id":"8a70abe6-77e8-49a4-b277-75bbf384b6bb","metadata":{},"outputs":[],"source":["Given the comparison of the mean squared errors, we can conclude that Batch Normalization works best for the simulated dataset. However, one takeaway I hope you to have is that for model training or machine learning in general, there are no cookie-cutter solutions for issues such as overfitting. There are no strict rules as to which technique is most effective when working with one type of dataset, so I would suggest you just try these techniques out and pick the one that you deem is the best (based on an evaluation metric) for your problem!\n","\n","Now, let's move on to a real world dataset.\n"]},{"cell_type":"markdown","id":"31935012-c679-472f-b5e4-b4209b35c9cd","metadata":{},"outputs":[],"source":["## Example 2: Spam Classification\n","\n","In this example, we will dive into a real world application, which is a spam classification problem. We will work with the [SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) from Kaggle and use neural networks for classifying a message as either Spam or Ham (not spam).\n","\n","We first download and display the raw dataset:\n"]},{"cell_type":"code","id":"c9732322-a0d1-4e15-abc6-d6ad69622c20","metadata":{},"outputs":[],"source":["import skillsnetwork\n\nawait skillsnetwork.download(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L3/data/spam.csv\")"]},{"cell_type":"code","id":"7440c946-f4e6-45dc-9a97-ea0ad8e9e8e7","metadata":{},"outputs":[],"source":["data = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\ndata.head()"]},{"cell_type":"markdown","id":"875d428e-543e-4c1a-8244-bceebfbe944e","metadata":{},"outputs":[],"source":["By calling the helper function **prepare_data**, we obtain a matrix $\\boldsymbol X$ which contains the pre-processed text data (mainly using the **TfidfVectorizer** from scikit-learn's **feature_extraction** module) and an object $\\boldsymbol y$ which contains the labels of the messages. \n","\n","There are 5572 text messages that will be classified as either spam **(1)** or non-spam **(0)**, and they are 8536-dimensional data.\n"]},{"cell_type":"code","id":"2b737b10-b2e8-449d-8871-7596c780e213","metadata":{},"outputs":[],"source":["X, y = prepare_data()\nX.shape, y.shape"]},{"cell_type":"markdown","id":"cd8636e8-d8d7-49a5-948f-d0d535d6fa75","metadata":{},"outputs":[],"source":["### Model Configuration and Training\n"]},{"cell_type":"markdown","id":"1f6ed57d-5df5-48f4-85f5-c6e2841c3bd0","metadata":{},"outputs":[],"source":["The **get_model** function provides the layer configuration ane the training of a neural network for different specifications of the regularization technique used. For example, by setting `reg='L1'`, the network's Dense layers will be regularized by L1, and then at the end of the training, the loss and accuracy curve of the train and validation set will be plotted.\n"]},{"cell_type":"code","id":"222bc393-ea2a-4d9e-a2ec-70d2ddbce584","metadata":{},"outputs":[],"source":["input_dim = X.shape[1]\n   \ndef get_model(reg=None, epochs=10):\n    model = Sequential()\n    model.add(Dense(512, activation='relu', input_shape=(input_dim,)))\n    if reg==\"L1\":\n        model.add(Dense(256, activation='relu', kernel_regularizer=\"l1\"))\n        model.add(Dense(64, activation='relu', kernel_regularizer=\"l1\"))\n    elif reg==\"L2\":\n        model.add(Dense(256, activation='relu', kernel_regularizer=\"l2\"))\n        model.add(Dense(64, activation='relu', kernel_regularizer=\"l2\"))\n    elif reg==\"Dropout\":\n        model.add(Dropout(0.2))\n        model.add(Dense(256, activation='relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(64, activation='relu'))\n        model.add(Dropout(0.2))\n    elif reg==\"BatchNorm\":\n        model.add(BatchNormalization())\n        model.add(Dense(256, activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dense(64, activation='relu'))\n        model.add(BatchNormalization())  \n    else:\n        model.add(Dense(256, activation='relu'))\n        model.add(Dense(64, activation='relu'))  \n \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n                 metrics=[\"accuracy\"])\n    history = model.fit(X, y, batch_size=64, validation_split=0.2,\n              epochs=epochs)\n    plot_metrics(history)\n"]},{"cell_type":"markdown","id":"e6101cec-7246-422c-ab1e-423a16f1c287","metadata":{},"outputs":[],"source":["### Baseline: No Regularization\n"]},{"cell_type":"code","id":"88d2a50a-5ba9-453d-8979-99ab7edd8abc","metadata":{},"outputs":[],"source":["get_model()"]},{"cell_type":"markdown","id":"ad55b2c6-4c30-4f2c-995b-b9551599702f","metadata":{},"outputs":[],"source":["You can see that without regularization, our training set accuracy is generally higher than validation accuracy, which means the model is overfitted to the training set.\n"]},{"cell_type":"markdown","id":"f7bf4549-676e-4301-b598-b6b2cde408cb","metadata":{},"outputs":[],"source":["### L1 Regularization\n"]},{"cell_type":"code","id":"e0dd3c22-e43d-4e8d-bc53-d158c09b5b5e","metadata":{},"outputs":[],"source":["get_model(reg=\"L1\")"]},{"cell_type":"markdown","id":"a9822ba2-46ca-466d-a8dd-9e414c4dcea8","metadata":{},"outputs":[],"source":["With L1 regularization, we can see that the validation accuracy is now higher than the training accuracy, meaning the L1 regularization mitigated the overfitting issue. However, if we look closer at the y-axis, we can see that in fact the accuracy value in general (both train and validation) is lower than the baseline model. \n","\n","For the baseline model, the validation accuracy was always above 0.97 even though it's slightly lower than the training accuracy. This indicates that, by using the default L1 regularization with shrinkage strength equals 0.001, our model is underfitting.\n"]},{"cell_type":"markdown","id":"11c6b37b-0ab9-4f3e-a309-aebbd5bfa40c","metadata":{},"outputs":[],"source":["### L2 Regularization\n"]},{"cell_type":"code","id":"8b89a46e-a457-4e9f-98e9-1e8cae65c6dc","metadata":{},"outputs":[],"source":["get_model(reg=\"L2\")"]},{"cell_type":"markdown","id":"759bb6c9-8cce-4a63-a8f8-7021c4946eb3","metadata":{},"outputs":[],"source":["With L2 regularization, the gap between training accuracy and validation accuracy narrowed a little bit. The training accuracy was not always 1.0 compared to the baseline and the validation accuracy had more fluctuations than the baseline.\n"]},{"cell_type":"markdown","id":"dd085ac7-a032-4dd6-8f83-74566b9bd029","metadata":{},"outputs":[],"source":["### Dropout\n"]},{"cell_type":"code","id":"869bc0b1-942e-45e8-91f8-589d85db3dfd","metadata":{},"outputs":[],"source":["get_model(reg=\"Dropout\")"]},{"cell_type":"markdown","id":"07dd9d2f-5696-4231-b39a-fd01ccd944cf","metadata":{},"outputs":[],"source":["With dropout, the resulting curves are very similar to the baseline, so perhaps dropout is not very effective in our case.\n"]},{"cell_type":"markdown","id":"26bddcf5-d475-4fd4-bfb4-e58f9518d4a9","metadata":{},"outputs":[],"source":["### Batch Normalization\n"]},{"cell_type":"code","id":"8e029345-af44-462b-a03d-a1da0966c5be","metadata":{},"outputs":[],"source":["get_model(reg=\"BathNorm\")"]},{"cell_type":"markdown","id":"52364a1c-3c6f-47c4-a5c5-21ab326a111d","metadata":{},"outputs":[],"source":["With Batch Norm, the final training accuracy is 1.0 and final validation accuracy is 0.9812, which is slightly higher than before (0.9794). We can conclude that Batch Normalization is a relatively better choice for regularizing neural network training.\n"]},{"cell_type":"markdown","id":"0dcbb68a-cb4d-48bf-87ac-9af6f510d68f","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"bc016e00-9c3b-4865-ab3a-cabe29eee8fa","metadata":{},"outputs":[],"source":["[Roxanne Li](https://www.linkedin.com/in/roxanne-li/) is a Data Scientist at IBM Skills Network.\n"]},{"cell_type":"markdown","id":"2d99381d-3804-4c79-a39a-f1317a4e829b","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"12c0a1cd-8561-495f-bf83-f28d00c4cdfc","metadata":{},"outputs":[],"source":["| Date (YYYY-MM-DD) | Version | Changed By  | Change Description |\n","| ----------------- | ------- | ----------- | ------------------ |\n","| 2022-07-18      | 0.1     | Roxanne Li  | Created Lab|\n","| 2022-09-07      | 0.1     | Steve Hord  | QA pass edits|\n"]},{"cell_type":"markdown","id":"31009618-eca4-4d3b-aa82-83310caca073","metadata":{},"outputs":[],"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"e19ff898fd7547a95bfdb5b2a71cdb5814cb7c6d54314a9eeec6570eda3fc3c3"},"nbformat":4,"nbformat_minor":4}