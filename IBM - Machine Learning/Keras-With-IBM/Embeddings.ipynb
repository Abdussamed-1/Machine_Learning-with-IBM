{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "# __Embeddings__\n",
    "\n",
    "Estimated time needed: **35** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You work for a streaming service that wants an automatic way to determine if they should add a movie to its catalog; each film is expensive to purchase, so the  streaming service would only like good movies to add to the catalog. You have been hired to build a neural network to determine if a movie is good or not using critics' written ratings similar to \"Rotten Tomatoes\". \n",
    "\n",
    "In this notebook, you will learn the fundamentals of how to convert text data to usable features for your neural network. In addition, you will also learn about the embedding layer, an engineering solution that allows you to implement linear layers for categorical data effectively. \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/movie_review.webp\" style=\"width: 60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    " - Understand the theory behind Embedding Layers  \n",
    " - Apply Tokenizer, Embedding Layers to a neural network \n",
    " - Perform Sentiment Analysis using a neural network \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the corresponding code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\samet\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Devingen bağlantı kitaplığını (DLL) başlatma işlemi başarısız.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: Devingen bağlantı kitaplığını (DLL) başlatma işlemi başarısız.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed\n\u001b[0;32m     12\u001b[0m seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m  \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     16\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m1234\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, Dense, Flatten,Dropout\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\samet\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Devingen bağlantı kitaplığını (DLL) başlatma işlemi başarısız.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import  tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Print tensorflow version, should be greater than 2.9.0\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Print tensorflow version, should be greater than 2.9.0\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Helper Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot the neural network's loss and accuracy curves from its training history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(history):\n",
    "\n",
    "    n = len(history.history[\"loss\"])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(range(n), (history.history[\"loss\"]),'r', label=\"Train Loss\")\n",
    "    ax.plot(range(n), (history.history[\"val_loss\"]),'b', label=\"Validation Loss\")\n",
    "    ax.legend()\n",
    "    ax.set_title('Loss over iterations')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(range(n), (history.history[\"acc\"]),'r', label=\"Train Acc\")\n",
    "    ax.plot(range(n), (history.history[\"val_acc\"]),'b', label=\"Validation Acc\")\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_title('Accuracy over iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot word embeddings that have been reduced to two dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(X_embedded,start=100,stop=300,sample=10):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X_embedded[start:stop:sample,0], X_embedded[start:stop:sample,1])\n",
    "    for i in range(start,stop,sample):\n",
    "        ax.annotate(REVERSE_LOOKUP[i+1], (X_embedded[i,0], X_embedded[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will convert a probability to a Rotten Tomatoes score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotten_tomato_score(p_yx):\n",
    "    return [\"rotten\"  if p<=0.60 else \"fresh\"for p in p_yx ]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As most machine learning models need their input variables to be numeric, text variables need to be transformed in the pre-processing part. There are several methods to perform this transformation. One-hot and Multi-hot encoding are frequently used to deal with textual data. \n",
    "\n",
    "One-hot encoding involves creating a set of vectors whose length equals the number of unique elements in the corpus or the vocabulary. We represent individual words as ones; else, the element is zero. Consider the following list of three **sequences**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=['I hate cats', \n",
    "         'the dog is brown and I like cats', \n",
    "         'for the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three **sequences**, and the corresponding number of **word tokens** for each sequence is given by:\n",
    "\n",
    "\n",
    "1. I hate cats - 3 tokens \n",
    "\n",
    "2. the dog is brown and likes cats - 8 tokens\n",
    "\n",
    "3. for the - 2 tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to encode text features to numeric features is using <b>one-hot-encoding</b> or <b>multi-hot-encoding</b>. The one-hot and multi-hot-encoding vector is a vector consisting of all zeros; if the sequence of text contains a specific word, then an element of the vector is set to one. Consider the vocabulary that consists of all the words in the list ```samples``` each vector will have an element assigned to it shown here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/one_hot.png\" width=\"200\" height=\"250\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has an additional component for words not in the vocabulary that we will leave out in this explanation. We have ten words; therefore, the vector will have ten elements. For example, we have the vector for the word ```I``` given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/I.png\" width=\"550\" height=\"650\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the elements in the vector are zero except for the index corresponding to the word ```I```; this is an example of one-hot-encoding. For **multi-hot-encoding** the sentence 'I hate cats', the vector is given by the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/I_hate_cats.png\" width=\"550\" height=\"650\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also represent the sequence as three one-hot-encoding vectors. We see that for each word that occurs, the value in the vector is set to one; else, they are set to zero; the dimension of both encoding vectors is equal to the number of unique words in the corpus (vocabulary). \n",
    "\n",
    "Lets see how to convert a sequence of multi-hot-encoding vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Embedding\n",
    "\n",
    "An embedding layer in Keras can be used when we want to create the embedding. These embeddings are learnable parameters that embed high dimensional data into lower dimensional vectors. Lets assume we have the embedding for the vocabulary above, we can represent the vocabulary as a list of parameter vectors $[w_{I}, w_{cats}, w_{the}, w_{hate}, w_{dog}, w_{is}, w_{brown},w_{and}, w_{like }, w_{for} ] $ .   \n",
    "\n",
    "Thus, we can obtain the embedding for the sentence **\"I hate cats\"** by first performing one-hot-encoding for each token (as shown on the right) and then performing the matrix multiplication with the parameter vectors on the left: \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/Ihat_cats_embeding.png\" width=\"1000\" height=\"1200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent the matrix as a set of three **one-hot-encoded** column vectors. We see the resultant vector is 1x3 **(output dimension x input length)** as Keras transposes this result. The input length is the number of words in a sentence. In this case, the output dimension is one, but it is usually a free parameter that you set as the number of dimensions of the low-dimensional space. \n",
    "\n",
    "We see that the operation is wasteful as we are multiplying many vectors by zeros. In real world applications, the one-hot-encoding is thousands of dimensions long. As a result, we use an **embedding layer**; this layer simply **outputs the parameters given the indexes**, as shown below. We can use these embeddings as the input to a neural network. Note that the parameters of an embedding layer are randomly initialized and are updated when training the neural network, just like the fully connected layers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/I_hate_cats_embeding.png\" width=\"1000\" height=\"1200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer in Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has the class ``Tokenizer`` that allows you to vectorize a text by turning each word into either a sequence of integers or a vector where the coefficient for each token is binary, based on word count. We will use the following parameters:\n",
    "\n",
    "```num_words```: the maximum number of words to keep in a sentence, based on the highest word frequencies, that is, only the **most common** ``num_words`` words will be kept.\n",
    "\n",
    "```filters```: a string that contains characters that will be filtered from the texts. The default is all the punctuations, tabs, line breaks, and the ' character.\n",
    "\n",
    "```lower```: boolean type, indicates whether to convert the texts to lowercase.\n",
    "\n",
    "```split```: str, indicates the separator for word splitting.\n",
    "\n",
    "We create the ```Tokenizer``` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the tokenizer to our text data, we call the method ```fit_on_texts```. The layer will build a corpus (vocabulary) of all string tokens obtained from the samples, and each word in the vocabulary will be represented by an integer or a vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(samples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute ```word_counts``` is a list of words, tokens, and the corresponding number of times they appeared in the samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts=tokenizer.word_counts\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the method ```texts_to_matrix``` to perform multi-hot encoding, where each element of the resultant vector is set to one if the corresponding word exists in the sequence; else, the element is set to zero. Here we perform multi-hot encoding for each word in the different sequences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tokenizer.word_counts.keys():\n",
    "    \n",
    "    print(key)\n",
    "    print(tokenizer.texts_to_matrix([key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is a total of ten words in the sequence, the vector has ten elements; each element in the vector corresponds to a different word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample  in samples:\n",
    "    \n",
    "    print(sample)\n",
    "    print(tokenizer.texts_to_matrix([sample]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply different NLP transformations using the parameter ```mode```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes=[ \"binary\", \"count\", \"tfidf\", \"freq\"]\n",
    "for mode in modes: \n",
    "    print(\"mode:\",mode)\n",
    "    for sample  in samples:\n",
    "        \n",
    "        print(sample)\n",
    "        print(tokenizer.texts_to_matrix([sample],mode=mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, Multi-hot encoding is redundant. As a result, we use the method ```texts_to_sequences```. This method will output an integer sequence where each integer corresponds to the index of the element in the corpus. The sequence length is equal to the number of words or tokens in the input sequence, as opposed to the number of words in the entire vocabulary like in Multi-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample  in samples:\n",
    "    \n",
    "    print(sample)\n",
    "    print(tokenizer.texts_to_sequences([sample]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Input Dimension \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example where we want to output the integer **input_dim**, which is the size of the vocabulary or maximum integer index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "input_dim=3\n",
    "output_dim=1\n",
    "input_length=1\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim,input_length=input_length))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras randomly initialize the weights, so let's set each weight to the index it belongs to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()\n",
    "weights=np.array([0,1,2]).reshape(-1,1)\n",
    "\n",
    "model.set_weights([weights])\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the function to each integer. The embedding layer will return the parameter value(s) or vector(s) on the position(s) which have the index value(s) equal to the integer(s) in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(3):\n",
    "    x=np.array([[n]])\n",
    "    print(\"input x={}\".format(n))\n",
    "    z=model.predict(x)\n",
    "    print(\"output z={}\".format(z.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply the method to multiple samples, by making each sample a different sample in the batch dimension or as multiple samples in a row:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.predict([[0],[1],[2]])\n",
    "print(\"different samples in the batch dimension:\\n\",z)\n",
    "z = model.predict([0,1,2])\n",
    "print(\" multiple samples in a row: \\n\",z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the ```Sequential``` class, you must specify the input shape ```input_length``` so the model can be built with the proper set of parameters; ```input_length``` is the size of each input sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output shape\n",
    "\n",
    "Output shape is the number of parameters, using the matrix multiplication analogy: if the input is a multi-hot encoding column vector and the embedding is a matrix, the ```output_dim``` is the number of columns in the matrix. Consider the example of converting a number to its binary number equivalent, and we will represent each element of the binary number as a different dimension in the output. \n",
    "\n",
    "Let's convert the numbers from 0 to 3. As we have four numbers the parameter ```input_dim=4```. As we need two binary digits or bits to represent the numbers from one to three the ```output_dim=2```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=4\n",
    "output_dim=2\n",
    "input_length=1\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign the ```weights``` to represent binary numbers;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "model.set_weights([weights])\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each input integer, the output of the embedding is the corresponding binary number:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(4):\n",
    "    x=np.array([[n]])\n",
    "    print(\"input x={}\".format(n))\n",
    "    z=model.predict(x)\n",
    "    print(\"input binary={}\".format(z.tolist() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to use the embedding, we go with seqeuntial \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, sequences are of different lengths. Consider our original sequence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=['I hate cats','the dog is brown and I like cats','for the']\n",
    "for sample in samples:\n",
    "    print(\"sample:\",sample)\n",
    "    print(\"length:\",len(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply the ```Tokenizer``` we apply the method ```texts_to_sequences``` we see each sequence is also a different length:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=12)\n",
    "tokenizer.fit_on_texts(samples) \n",
    "tokens=tokenizer.texts_to_sequences(samples)\n",
    "print(\"tokens\",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the sequences are the same length, we apply the function ```pad_sequences```. The function transforms a sequence into a 2D Numpy array with the number of rows equal to the number of samples and the number of columns equal to the parameter ```maxlen``` (the length of the longest sequence in the list). Sequences that are shorter than ```maxlen``` are padded with the parameter ```value```, which is by default zero. \n",
    "\n",
    "Consider the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=9\n",
    "x =pad_sequences(tokens, maxlen=maxlen,value=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the padding proceeds the integer values, but we can also set padding to follow the integers through ```padding = \"post\"```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=9\n",
    "x =pad_sequences(tokens, maxlen=maxlen,padding=\"post\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decrease the value for ```maxlen```, the function will ```truncate``` the sequences, that is, making sequences that are longer than `maxlen` shorter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=5\n",
    "x =pad_sequences(tokens, maxlen=maxlen)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the function truncates the values from sequences longer than ```maxlen``` at the beginning of the sequence. We can also set ```truncating = \"post\"```to  truncate the values from the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the sequence, , and embedding layers, we can use a neural network to classify our text data. Sentiment analysis in natural language processing is a technique used to determine opinions, appraisals, emotions, or attitudes towards a topic, person, or entity. \n",
    "\n",
    "Consider the IMDB dataset of 25,000 movie reviews from IMDB, labeled positive (y=0) and negative (y=1). In this dateset, the reviews have been pre-processed, and each review is encoded as a list of word indexes (integers). The words are indexed by overall frequency in the dataset so that for instance, the integer \"3\" encodes the 3rd most frequent word in the data. We will \"only consider the top 10,000 most common words, but eliminate the top 20 most common words for more on the dataset check out <a href=\"https://keras.io/api/datasets/imdb/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">here </a>. We download the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "\n",
    "# change the default parameter of np to allow_pickle=True\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "importlib.reload(np)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print out the first three samples and we see the dataset has been tokenized: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(x_train[0:3]):\n",
    "    print(\"Sequence:\",i) \n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain a dictionary look-up table with the word and the integer that represents it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invert the table so the key is the integer that represents the word, and the value is the word. We assign the new dictionary to the variable ```REVERSE_LOOKUP```:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVERSE_LOOKUP={value:key for key, value in word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will use ```REVERSE_LOOKUP``` to convert an input sequence x to the original review. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(x):\n",
    "     return' '.join([REVERSE_LOOKUP[index ] for index in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the review for the first sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_review(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we obtain the review for the second sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_review(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is of a different length; for example, we can print the length of the first three samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(x_train[0:3]):\n",
    "    print(\"length {} of sample {}:\".format(i,len(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we need to pad all the sequences using ```pad_sequences```:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=20\n",
    "x_train =pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =pad_sequences(x_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the embedding layer. We know the number of words is ```max_features=10000``` and therefore ```input_dim=10000```. The dimension of the output is a parameter that we set. Let's set ```output_dim=8``` and ```input_length=1``` for now.\n",
    "\n",
    "We also need to add a flattening layer using ```Flatten()```, which adds an extra dimension and will reshape the input as ```(batch, 1)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=20))\n",
    "model.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add a fully connected layer with `units=1` for classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10,batch_size=30,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the training history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is a probability as it ranges from 0 to 1, so we can use this as a proxy for the Rotten Tomatoes score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_yx=model.predict(x_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the the rating using the helper function ```rotten_tomato_score``` which maps a probability to a prediction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_tomato_score(p_yx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the embedding  weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights in the embedding layer contain correlations between words. Let's obtain the embedding weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10,000 words so there are 10,000 different parameter vectors, each has 8 dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't visualize the vector as it's 8-dimensional, but we can use **T-distributed Stochastic Neighbor Embedding (TSNE)** to reduce the dimension to two for visualizing the embeddings on a 2D plot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X_embedded = tsne.fit_transform(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the embeddings are transformed let's use the helper function ```plot_embedding``` to visualize them. The starting word index is ```start```, and ```stop``` is the index of the last word; ```sample``` is the number of samples between samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=1\n",
    "stop=600\n",
    "sample=10\n",
    "plot_embedding(X_embedded,start,stop,sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see words like \"war\", \"evil\", and \"killer\" are clustered in the lower-right quadrant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=1\n",
    "stop=100\n",
    "sample=1\n",
    "plot_embedding(X_embedded,start,stop,sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 \n",
    "\n",
    "Recreate the model above but add a dense layer with 500 neurons (using relu activation), a dense layer with 250 neurons (using relu activation), and a final layer for classification, then use the function ```display_metrics``` to plot the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=20))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10,batch_size=30,validation_split=0.2)\n",
    "\n",
    "display_metrics(history)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Question: Does this model perform better then the last model? Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "\n",
    "The second model suffers from over Overfitting\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Introduce L2 and dropout regulation to your model (there are many solutions, one is given here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, kernel_initializer='normal', activation='relu',kernel_regularizer=regularizers.L2(l2=5e-3)))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Dense(250, kernel_initializer='normal', activation='relu',kernel_regularizer=regularizers.L2(l2=5e-3)))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10,batch_size=64, validation_split=0.2)\n",
    "display_metrics(history)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have completed this lab!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author\n",
    "\n",
    "\n",
    "[Joseph Santarcangelo](https://www.linkedin.com/in/david-pasternak-6b84a2208/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                 |\n",
    "| ----------------- | ------- | ---------- | ---------------------------------- |\n",
    "| 2022-03-28        | 0.1     | David Pasternak      | Created Lab    |\n",
    "| 2022-05-10        | 0.2     | Sam Prokopchuk      | Complete Draft of Lab    |\n",
    "| 2022-08-10        | 0.2     | Roxanne Li      | Review and edit Lab    |\n",
    "| 2022-09-08        | 0.2     | Steve Hord      | QA pass and edits      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> © IBM Corporation 2022. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
