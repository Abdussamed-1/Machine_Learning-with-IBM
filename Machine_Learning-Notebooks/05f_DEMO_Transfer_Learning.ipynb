{"cells":[{"cell_type":"markdown","id":"3be91431-de79-4fa1-83ba-4a9e0c714062","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"d212edba-50ea-4f58-8b61-1d04c719d666","metadata":{},"outputs":[],"source":["# Machine Learning Foundation\n","\n","## Course 5, Part g: Transfer Learning DEMO\n"]},{"cell_type":"markdown","id":"54431efa-6e48-4943-be75-5c7077b61bc5","metadata":{},"outputs":[],"source":["For this exercise, we will use the well-known MNIST digit data. To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n","\n","\n"]},{"cell_type":"code","id":"86bba11e-3202-4e79-b56f-dde452902393","metadata":{},"outputs":[],"source":["import datetime\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n#from tensorflow import keras\n#from tensorflow.keras.datasets import mnist\n#from tensorflow.keras.models import Sequential\n#from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n#from tensorflow.keras.layers import Conv2D, MaxPooling2D\n#from tensorflow.keras import backend as K"]},{"cell_type":"code","id":"5b6d57e9-fbb5-4963-904e-89d645c943ad","metadata":{},"outputs":[],"source":["#used to help some of the timing functions\nnow = datetime.datetime.now"]},{"cell_type":"code","id":"e635a6d9-34ab-4684-aa98-6d8657d9cbd4","metadata":{},"outputs":[],"source":["# set some parameters\nbatch_size = 128\nnum_classes = 5\nepochs = 5"]},{"cell_type":"code","id":"08a42420-a6ef-4cc4-8252-a594225f145e","metadata":{},"outputs":[],"source":["# set some more parameters\nimg_rows, img_cols = 28, 28\nfilters = 32\npool_size = 2\nkernel_size = 3"]},{"cell_type":"code","id":"8585d00d-64cb-4365-9aa6-b0d5e2b49141","metadata":{},"outputs":[],"source":["## This just handles some variability in how the input data is loaded\n\nif K.image_data_format() == 'channels_first':\n    input_shape = (1, img_rows, img_cols)\nelse:\n    input_shape = (img_rows, img_cols, 1)"]},{"cell_type":"code","id":"9436f336-ac03-45ef-a809-d1c8ac84c341","metadata":{},"outputs":[],"source":["## To simplify things, write a function to include all the training steps\n## As input, function takes a model, training set, test set, and the number of classes\n## Inside the model object will be the state about which layers we are freezing and which we are training\n\ndef train_model(model, train, test, num_classes):\n    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train /= 255\n    x_test /= 255\n    print('x_train shape:', x_train.shape)\n    print(x_train.shape[0], 'train samples')\n    print(x_test.shape[0], 'test samples')\n\n    # convert class vectors to binary class matrices\n    y_train = keras.utils.to_categorical(train[1], num_classes)\n    y_test = keras.utils.to_categorical(test[1], num_classes)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adadelta',\n                  metrics=['accuracy'])\n\n    t = now()\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              verbose=1,\n              validation_data=(x_test, y_test))\n    print('Training time: %s' % (now() - t))\n\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print('Test score:', score[0])\n    print('Test accuracy:', score[1])"]},{"cell_type":"code","id":"007f441b-2e86-431f-bf64-35649b5e4165","metadata":{},"outputs":[],"source":["# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# create two datasets: one with digits below 5 and one with 5 and above\nx_train_lt5 = x_train[y_train < 5]\ny_train_lt5 = y_train[y_train < 5]\nx_test_lt5 = x_test[y_test < 5]\ny_test_lt5 = y_test[y_test < 5]\n\nx_train_gte5 = x_train[y_train >= 5]\ny_train_gte5 = y_train[y_train >= 5] - 5\nx_test_gte5 = x_test[y_test >= 5]\ny_test_gte5 = y_test[y_test >= 5] - 5"]},{"cell_type":"code","id":"2413bae1-9af4-4339-88aa-3bcbfffaaa35","metadata":{},"outputs":[],"source":["# Define the \"feature\" layers.  These are the early layers that we expect will \"transfer\"\n# to a new problem.  We will freeze these layers during the fine-tuning process\n\nfeature_layers = [\n    Conv2D(filters, kernel_size,\n           padding='valid',\n           input_shape=input_shape),\n    Activation('relu'),\n    Conv2D(filters, kernel_size),\n    Activation('relu'),\n    MaxPooling2D(pool_size=pool_size),\n    Dropout(0.25),\n    Flatten(),\n]"]},{"cell_type":"code","id":"dc604802-afce-4642-af0b-0a3675708bdf","metadata":{},"outputs":[],"source":["# Define the \"classification\" layers.  These are the later layers that predict the specific classes from the features\n# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n\nclassification_layers = [\n    Dense(128),\n    Activation('relu'),\n    Dropout(0.5),\n    Dense(num_classes),\n    Activation('softmax')\n]"]},{"cell_type":"code","id":"36c685cc-e358-4fc5-8599-862bff32f6c2","metadata":{},"outputs":[],"source":["# We create our model by combining the two sets of layers as follows\nmodel = Sequential(feature_layers + classification_layers)"]},{"cell_type":"code","id":"0fac4a56-fbb7-4204-8baf-572d96509d7c","metadata":{},"outputs":[],"source":["# Let's take a look\nmodel.summary()"]},{"cell_type":"code","id":"97c45cdb-c6f3-4aa7-9960-8c65f709ee5a","metadata":{},"outputs":[],"source":["# Now, let's train our model on the digits 5,6,7,8,9\n\ntrain_model(model,\n            (x_train_gte5, y_train_gte5),\n            (x_test_gte5, y_test_gte5), num_classes)"]},{"cell_type":"markdown","id":"e5834a04-d11e-42ae-bc1a-6e5ef5a5a362","metadata":{},"outputs":[],"source":["### Freezing Layers\n","Keras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n","\n","Note also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set.\n"]},{"cell_type":"code","id":"ecbb5fd3-69e1-4c8c-b9fd-d489b7edeeec","metadata":{},"outputs":[],"source":["# Freeze only the feature layers\nfor l in feature_layers:\n    l.trainable = False"]},{"cell_type":"markdown","id":"f7d9d95f-318b-49ad-a44c-998117ae4eab","metadata":{},"outputs":[],"source":["Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*.\n"]},{"cell_type":"code","id":"b08739e4-31e5-4cdd-ac24-617cde46d31e","metadata":{},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","id":"c3ca2ae0-0b2d-4ccd-8c5e-ec0c63189549","metadata":{},"outputs":[],"source":["train_model(model,\n            (x_train_lt5, y_train_lt5),\n            (x_test_lt5, y_test_lt5), num_classes)"]},{"cell_type":"markdown","id":"3d2416d5-bd8a-487a-a165-f9c02fb094f5","metadata":{},"outputs":[],"source":["Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n","\n","Also, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster. \n"]},{"cell_type":"markdown","id":"3f7d824d-2f4d-4219-8e82-1409cf46fbb3","metadata":{},"outputs":[],"source":["## Exercise\n","- Now we will write code to reverse this training process.  That is, train on the digits 0-4, then finetune only the last layers on the digits 5-9.\n"]},{"cell_type":"code","id":"11b64a6d-ada5-470e-a5ee-c4fe0cfcc99c","metadata":{},"outputs":[],"source":["# Create layers and define the model as above\nfeature_layers2 = [\n    Conv2D(filters, kernel_size,\n           padding='valid',\n           input_shape=input_shape),\n    Activation('relu'),\n    Conv2D(filters, kernel_size),\n    Activation('relu'),\n    MaxPooling2D(pool_size=pool_size),\n    Dropout(0.25),\n    Flatten(),\n]\n\nclassification_layers2 = [\n    Dense(128),\n    Activation('relu'),\n    Dropout(0.5),\n    Dense(num_classes),\n    Activation('softmax')\n]\nmodel2 = Sequential(feature_layers2 + classification_layers2)\nmodel2.summary()"]},{"cell_type":"code","id":"598ca2d3-c94f-4f9f-9e00-71bd6a6a24db","metadata":{},"outputs":[],"source":["# Now, let's train our model on the digits 0,1,2,3,4\ntrain_model(model2,\n            (x_train_lt5, y_train_lt5),\n            (x_test_lt5, y_test_lt5), num_classes)"]},{"cell_type":"code","id":"14469120-a5df-4996-8e79-9d117e844bec","metadata":{},"outputs":[],"source":["#Freeze layers\nfor l in feature_layers2:\n    l.trainable = False"]},{"cell_type":"code","id":"dff5dae1-c4e9-4ea8-873f-8de3fba400cf","metadata":{},"outputs":[],"source":["model2.summary()"]},{"cell_type":"code","id":"9cdae53c-3f9a-4dc6-999c-49b25cc27ffc","metadata":{},"outputs":[],"source":["train_model(model2,\n            (x_train_gte5, y_train_gte5),\n            (x_test_gte5, y_test_gte5), num_classes)"]},{"cell_type":"markdown","id":"e7c6b580-4ae7-4593-a9f4-3a6b4f08c665","metadata":{},"outputs":[],"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"f07d0353263164e3930b6a97c8ca1b6a57cd8218a57a2544558d5938040d4d3b"},"nbformat":4,"nbformat_minor":4}